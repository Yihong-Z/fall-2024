[
  {
    "objectID": "howtos/windowsAndLinux.html",
    "href": "howtos/windowsAndLinux.html",
    "title": "Windows 10 and the Ubuntu Subsystem",
    "section": "",
    "text": "Windows 10 has a powerful new feature that allows a full Linux system to be installed and run from within Windows. This is incredibly useful for building/testing code in Linux, without having a dedicated Linux machine, but it poses strange new behaviors as two very different operating systems coexist in one place. Initially, this document mirrors the Windows Install tutorial, showing you how to install Ubuntu and setting up R, RStudio, and LaTex. Then, we cover some of the issues of running two systems together, starting with finding files, finding the Ubuntu subsystem, and file modifications.",
    "crumbs": [
      "How tos",
      "Windows 10 and the Ubuntu Subsystem"
    ]
  },
  {
    "objectID": "howtos/windowsAndLinux.html#installing-ubuntu",
    "href": "howtos/windowsAndLinux.html#installing-ubuntu",
    "title": "Windows 10 and the Ubuntu Subsystem",
    "section": "Installing Ubuntu",
    "text": "Installing Ubuntu\nThere are 2 parts to installing a Linux subsystem in Windows. I will write this using Ubuntu as the example, as it is my preferred Linux distro, but several others are provided by Windows.\nSources:\n\nOfficial Windows Instructions\nUbuntu Update Instructions\n\n\n1) Enable Linux Subsystem\nBy default, the Linux subsystem is an optional addition in Windows. This feature has to be enabled prior to installing Linux. There are two ways to do it.\n\nCMD Line\nThe simplest way to enable the Linux subsystem is through PowerShell.\n\nOpen PowerShell as Administrator\nRun the following (on one line):\nEnable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux\nRestart the computer\n\nGUI\nIf you don’t wish to use PowerShell, you can work your way through the control panel and turn on the Linux subsystem.\n\nOpen the settings page through the search bar\nGo to Programs and Features\nFind Turn Windows Features on or off on the right side\nEnable the Windows Subsystem for Linux option\nRestart the computer\n\n\n\n\n2) Install Linux Subsystem\nOnce the Linux subsystem feature has been enabled, there are multiple methods to download and install the Linux distro you want. I highly recommend installing Ubuntu from the Microsoft store. There are several other flavors available as well, but Ubuntu is generally the easiest to learn and the most well supported.\n\nOpen the Microsoft Store\nSearch for Ubuntu\n\nYou’re looking for the highest number followed by LTS, currently 20.04 LTS (or 18.04 LTS is fine too). This is the current long-term-release, meaning it will be supported for the next 5 years.\n\nClick on the tile, then click Get, and this should start the installation.\nFollow the prompts to install Ubuntu.\n\nAfter installing Ubuntu, it is advisable to update it. This is something you should do on a regular basis.\n\nOpen a Bash terminal.\nType sudo apt update to update your local package database.\nType sudo apt upgrade to upgrade your installed packages.",
    "crumbs": [
      "How tos",
      "Windows 10 and the Ubuntu Subsystem"
    ]
  },
  {
    "objectID": "howtos/windowsAndLinux.html#using-the-linux-terminal-from-r-in-windows",
    "href": "howtos/windowsAndLinux.html#using-the-linux-terminal-from-r-in-windows",
    "title": "Windows 10 and the Ubuntu Subsystem",
    "section": "Using the Linux Terminal from R in Windows",
    "text": "Using the Linux Terminal from R in Windows\nTo get all the functionality of a UNIX-style commandline from within R (e.g., for bash code chunks), you should set the terminal under R in Windows to be the Linux subsystem.",
    "crumbs": [
      "How tos",
      "Windows 10 and the Ubuntu Subsystem"
    ]
  },
  {
    "objectID": "howtos/windowsAndLinux.html#a-note-on-file-modification",
    "href": "howtos/windowsAndLinux.html#a-note-on-file-modification",
    "title": "Windows 10 and the Ubuntu Subsystem",
    "section": "A Note on File Modification",
    "text": "A Note on File Modification\nDO NOT MODIFY LINUX FILES FROM WINDOWS\nIt is highly recommended that you never modify Linux files from Windows because of metadata corruption issues. Any files created under the Linux subsystem, only modify them with Linux tools. In contrast, you can create files in the Windows system and modify them with both Windows or Linux tools. There could be file permission issues because Windows doesn’t have the same concept of file permissions as Linux. So, if you intend to work on files using both Linux and Windows, create the files in the C drive under Windows, and you should be safe to edit them with either OS.",
    "crumbs": [
      "How tos",
      "Windows 10 and the Ubuntu Subsystem"
    ]
  },
  {
    "objectID": "howtos/windowsAndLinux.html#finding-windows-from-linux",
    "href": "howtos/windowsAndLinux.html#finding-windows-from-linux",
    "title": "Windows 10 and the Ubuntu Subsystem",
    "section": "Finding Windows from Linux",
    "text": "Finding Windows from Linux\nOnce you have some flavor of Linux installed, you need to be able to navigate from your Linux home directory to wherever Windows stores files. This is relatively simple, as the Windows Subsystem shows Windows to Linux as a mounted drive.\n\nSource\n\n\nOpen a Bash terminal.\nType cd / to get to the root directory.\nIn root, type cd /mnt. This gets you to the mount point for your drives.\nType ls to see what drives are available (you should see a c, and maybe d as well).\nType cd c to get into the Windows C drive. This is the root of the C directory for Windows.\nTo find your files, change directy into the users folder, then into your username.\n\ncd Users/&lt;your-user-name&gt;\nThis is your home directory in Windows. If you type ls here, you should see things like\n\nDocuments\nDownloads\nPictures\nVideos\netc…",
    "crumbs": [
      "How tos",
      "Windows 10 and the Ubuntu Subsystem"
    ]
  },
  {
    "objectID": "howtos/windowsAndLinux.html#finding-linux-from-windows",
    "href": "howtos/windowsAndLinux.html#finding-linux-from-windows",
    "title": "Windows 10 and the Ubuntu Subsystem",
    "section": "Finding Linux from Windows",
    "text": "Finding Linux from Windows\nThis is slightly more tricky than getting from Linux to Windows. Windows stores the Linux files in a hidden subfolder so that you don’t mess with them from Windows. However, you can find them, and then the easiest way (note, do not read as safest or smartest) to find those files in the future is by creating a desktop shortcut.\n\nSource\n\n\nOpen File Explorer\nIn the address bar, type %userprofile%\\AppData\\Local\\Packages\n\n%userprofile% will expand to something like C:\\Users\\&lt;your-user-name&gt;\n\nLook for a folder related to the Linux distro that you installed\n\nThese names will change slightly over time, but look for something similar-ish.\nFor Ubuntu, look for something with CanonicalGroupLimited.UbuntuonWindows in it.\n\nCanonical is the creator/distributor of Ubuntu.\n\n\nClick LocalState\nClick rootfs\n\nThis is the root of your Linux distro.\n\nClick into home and then into your user name.\n\nThis is your home directory under Linux.\n\nDO NOT MODIFY THESE FILES FROM WINDOWS\n\nData corruption is a possibility.\n\n\nSo, the final path to find your home directory from windows will look like:\n%userprofile%\\AppData\\Local\\Packages\\&lt;Distro-Folder&gt;\\LocalStat\\rootfs\\home\\&lt;your-user-name&gt;\\",
    "crumbs": [
      "How tos",
      "Windows 10 and the Ubuntu Subsystem"
    ]
  },
  {
    "objectID": "howtos/windowsAndLinux.html#installing-r-on-the-linux-subsystem",
    "href": "howtos/windowsAndLinux.html#installing-r-on-the-linux-subsystem",
    "title": "Windows 10 and the Ubuntu Subsystem",
    "section": "Installing R on the Linux Subsystem",
    "text": "Installing R on the Linux Subsystem\nIMPORTANT: This section is only if you’d like to try using R under Linux. For class, using R under Windows should be fine.\nThe Linux Subsystem behaves exactly like a regular Linux installation, but for completeness, I will provide instructions here for people new to Linux. These instructions are written from the perspective of Ubuntu, but will be similar for other repos.\nR is not a part of the standard Ubuntu installation. So, we have to add the repository manually to our repository list. This is relatively straightforward, and R supports several versions of Ubuntu.\nSources:\n\nCRAN guide for Ubuntu\nDigital Ocean quick tutorial\n\n\nIn a bash window, type:\nsudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E298A3A825C0D65DFD57CBB651716619E084DAB9\n\nThis adds the key to “sign”, or validate, the R repository\n\nThen, type:\nsudo add-apt-repository 'deb https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/'\n\ncloud.r-project.org is the default mirror, however, it is prudent to connect to the mirror closest to you geographically. Berkeley has it’s own mirror, so the command with the Berkeley mirror would look like\n\nsudo add-apt-repository 'deb https://cran.r-project.org/bin/linux/ubuntu/bionic-cran40/'\nFinally, type sudo apt install r-base, and press y to confirm installation\nTo test that it worked, type R into the console, and an R session should begin\n\nType q() to quit the R session",
    "crumbs": [
      "How tos",
      "Windows 10 and the Ubuntu Subsystem"
    ]
  },
  {
    "objectID": "howtos/windowsAndLinux.html#installing-rstudio-on-the-linux-subsystem",
    "href": "howtos/windowsAndLinux.html#installing-rstudio-on-the-linux-subsystem",
    "title": "Windows 10 and the Ubuntu Subsystem",
    "section": "Installing Rstudio on the Linux Subsystem",
    "text": "Installing Rstudio on the Linux Subsystem\n\n\n\n\n\n\nWarning\n\n\n\nTHIS NO LONGER WORKS\n\n\nAs of Rstudio 1.5.x, it does not run on WSL. Link\nAlso possible issues, WSL has no GUI, and therefore can’t support anything that uses a GUI.\nThese instructions work, but Rstudio doesn’t run.\nSources:\n\nRstudio\nSource\n\n\nGo the the Rstudio website (link above) and download the appropriate Rstudio Desktop version.\n\nFor most people, this is the Ubuntu 18 (64-bit) installer.\nSave it somewhere that you can find it.\nYou should have a file similar to rstudio-&lt;version number&gt;-amd64.deb\n\nOpen a terminal window and navigate to wherever you saved the rstudio install file.\nType the command sudo dpkg -i ./rstudio-&lt;version number&gt;-amd64.deb\n\nThis tells the package installer (dpkg) to install (-i) the file specified (./thing.deb)\n\nType the command sudo apt-get install -f\n\nThis tells the package manager (apt-get) to fix (-f) any dependency issues that may have arisen when installing the package.\n\nType the command which rstudio to make sure the system can find it.\n\nOutput should be similar to /usr/bin/rstudio\n\nRun rstudio from linux by typing rstudio &\n\nThe & runs it in the background, allowing you to close the terminal window.",
    "crumbs": [
      "How tos",
      "Windows 10 and the Ubuntu Subsystem"
    ]
  },
  {
    "objectID": "howtos/windowsAndLinux.html#installing-latex-on-the-linux-subsystem",
    "href": "howtos/windowsAndLinux.html#installing-latex-on-the-linux-subsystem",
    "title": "Windows 10 and the Ubuntu Subsystem",
    "section": "Installing LaTeX on the Linux Subsystem",
    "text": "Installing LaTeX on the Linux Subsystem\n\n\n\n\n\n\nImportant\n\n\n\nThis section is only if you’d like to try using LaTeX under Linux. For class, using LaTeX (or R Markdown) under Windows should be fine.\n\n\nLaTeX is a text-markup language used when generating documents from .Rmd files.\nSource LaTeX\n\nType sudo apt-get install texlive-full, press y to confirm installation\n\nGenerally, if you want to create and edit R Markdown documents you will also need a text editor to go with your LaTeX installation, but we won’t go into that here.",
    "crumbs": [
      "How tos",
      "Windows 10 and the Ubuntu Subsystem"
    ]
  },
  {
    "objectID": "howtos/installGit.html",
    "href": "howtos/installGit.html",
    "title": "Installing Git",
    "section": "",
    "text": "Here are some instructions for installing Git on your computer. Git is the version control software we’ll use in the course.\nYou can install Git by downloading and installing the correct binary from here.\nFor macOS, deeb recommends using the Homebrew option.\nGit comes installed on the SCF, so if you login to an SCF machine and want to use Git there, you don’t need to install Git.\n\nSidenotes on using Git with RStudio\nYou can work with Git through RStudio via RStudio projects.\nHere are some instructions. Here are some helpful guidelines from RStudio.\nYou may need to tell RStudio where the Git executable is located as follows.\n\nOn Windows, the git executable should be installed somewhere like: \"C:/Program Files (x86)/Git/bin/git.exe\"\nOn MacOS X, you can locate the executable by executing the following in Terminal: which git\nOnce you locate the executable, you may then need to confirm that RStudio is looking in the right place. Go to “Tools -&gt; Options -&gt; Git/SVN -&gt; Git executable” and confirm it has the correct information about the location of the git executable.",
    "crumbs": [
      "How tos",
      "Installing Git"
    ]
  },
  {
    "objectID": "howtos/accessPython.html",
    "href": "howtos/accessPython.html",
    "title": "Accessing Python",
    "section": "",
    "text": "We recommend using the Miniforge distribution as your Python 3.12 installation.\nOnce you’ve installed Python, please install the following packages:\nAssuming you installed Miniforge, you should be able to do this from the command line:\nWhile you’re welcome to work with Python in a Jupyter notebook for exploration (e.g., using the campus DataHub, you’ll need to submit Quarto (.qmd) documents with Python chunks for your problem sets. So you’ll need Python set up on your laptop or to use it by logging in to an SCF machine.",
    "crumbs": [
      "How tos",
      "Accessing Python"
    ]
  },
  {
    "objectID": "howtos/accessPython.html#python-from-the-command-line",
    "href": "howtos/accessPython.html#python-from-the-command-line",
    "title": "Accessing Python",
    "section": "Python from the command line",
    "text": "Python from the command line\nOnce you get your SCF account, you can access Python or IPython from the UNIX command line as soon as you login to an SCF server. Just SSH to an SCF Linux machine (e.g., gandalf.berkeley.edu or radagast.berkeley.edu) and run ‘python’ or ‘ipython’ from the command line.\nMore details on using SSH are here. Note that if you have the Ubuntu subsystem for Windows, you can use SSH directly from the Ubuntu terminal.",
    "crumbs": [
      "How tos",
      "Accessing Python"
    ]
  },
  {
    "objectID": "howtos/accessPython.html#python-via-jupyter-notebook",
    "href": "howtos/accessPython.html#python-via-jupyter-notebook",
    "title": "Accessing Python",
    "section": "Python via Jupyter notebook",
    "text": "Python via Jupyter notebook\nYou can use a Jupyter notebook to run Python code from the SCF JupyterHub or the Berkeley DataHub.\nIf you’re on the SCF JupyterHub, select Start My Server. Then, unless you are running long or parallelized code, just click Spawn (in other words, accept the default ‘standalone’ partition). On the next page select ‘New’ and ‘Python 3’.\nTo finish your session, click on Control Panel and Stop My Server. Do not click Logout.",
    "crumbs": [
      "How tos",
      "Accessing Python"
    ]
  },
  {
    "objectID": "howtos/accessUnixCommandLine.html",
    "href": "howtos/accessUnixCommandLine.html",
    "title": "Accessing the Unix Command Line",
    "section": "",
    "text": "You have several options for UNIX command-line access. You’ll need to choose one of these and get it working.\n\nMac OS (on your personal machine):\nOpen a Terminal by going to Applications -&gt; Utilities -&gt; Terminal\n\n\nWindows (on your personal machine):\n\nYou may be able to use the Ubuntu bash shell available in Windows.\nYour PC must be running a 64-bit version of Windows 10 Anniversary Update or later (build 1607+).\nPlease see these links for more information:\n\nhttp://blog.revolutionanalytics.com/2017/12/r-in-the-windows-subsystem-for-linux.html\nhttps://msdn.microsoft.com/en-us/commandline/wsl/install_guide\n\nFor more detailed instructions, see the Installing the Linux Subsystem on Windows tutorial.\n(Not recommended) There’s an older program called cygwin that provides a UNIX command-line interface.\n\nNote that when you install Git on Windows, you will get Git Bash. While you can use this to control Git, the functionality is limited so this will not be enough for general UNIX command-line access for the course.\n\n\nLinux (on your personal machine):\nIf you have access to a Linux machine, you very likely know how to access a terminal.\n\n\nAccess via DataHub (provided by UC Berkeley’s Data Science Education Program)\n\nGo to https://datahub.berkeley.edu\nClick on Sign in with bCourses, sign in via CalNet, and authorize DataHub to have access to your account.\nIn the mid-upper right, click on New and Terminal.\nTo end your session, click on Control Panel and Stop My Server. Note that Logout will not end your running session, it will just log you out of it.\n\n\n\nAccess via the Statistical Computing Facility (SCF)\nWith an SCF account (available here), you can access a bash shell in the ways listed below.\nThose of you in the Statistics Department should be in the process of getting an SCF account. Everyone else will need an SCF account when we get to the unit on parallel computing, but you can request an account now if you prefer.\n\nYou can login to our various Linux servers and access a bash shell that way. Please see http://statistics.berkeley.edu/computing/access.\nYou can also access a bash shell via the SCF JupyterHub interface; please see the Accessing Python instructions but when you click on New, choose Terminal. This is very similar to the DataHub functionality discussed above.",
    "crumbs": [
      "How tos",
      "Accessing the Unix Command Line"
    ]
  },
  {
    "objectID": "rubric.html",
    "href": "rubric.html",
    "title": "",
    "section": "",
    "text": "This document provides guidance for submitting high-quality problem set (and project) solutions. This guidance is based on general good practices for scientific communication, reproducible research, and software development."
  },
  {
    "objectID": "rubric.html#general-presentation",
    "href": "rubric.html#general-presentation",
    "title": "",
    "section": "General presentation",
    "text": "General presentation\n\nSimply presenting code or derivations is not sufficient.\nBriefly describe the overall goal or strategy before providing code/derivations.\nAs needed describe what the pieces of your code/derivation are doing to make it easier for a reader to follow the steps.\nKeep your output focused on illustrating what you did, without distracting from the flow of your solution by showing voluminous output. The output should illustrate and demonstrate, not overwhelm or obscure. If you need to show longer output, you can add it at the end as supplemental material.\nOutput should be produced by your code (i.e., from the code chunks running when the document is rendered), not by copying and pasting results into the document."
  },
  {
    "objectID": "rubric.html#coding-practice",
    "href": "rubric.html#coding-practice",
    "title": "",
    "section": "Coding practice",
    "text": "Coding practice\n\nMinimize (or eliminate) use of global variables.\nBreak down work into core tasks and develop small, modular, self-contained functions (or class methods) to carry out those tasks.\nDon’t repeat code. As needed refactor code to create new functions (or class methods).\nFunctions and classes should be “weakly coupled”, interacting via their interfaces and not by having to know the internals of how they work.\nUse data structures appropriate for the computations that need to be done.\nDon’t hard code ‘magic’ numbers. Assign such numbers to variables with clear names, e.g., speed_of_light = 3e8.\nProvide reasonable default arguments to functions (or class methods) when possible.\nProvide tests (including unit tests) when requested (this is good general practice but we won’t require it in all cases).\nAvoid overly complicated syntax – try to use the clearest syntax you can to solve the problem.\nIn terms of speed, don’t worry about it too much so long as the code finishes real-world tasks in a reasonable amount of time. When optimizing, focus on the parts of the code that are the bottlenecks.\nUse functions already available in the language rather than recreating yourself."
  },
  {
    "objectID": "rubric.html#code-style",
    "href": "rubric.html#code-style",
    "title": "",
    "section": "Code style",
    "text": "Code style\n\nFollow a consistent style. While you don’t have to follow Python’s PEP8 style guide exactly, please look at it and follow it generally.\nUse informative variable and function names and have a consistent naming style.\nUse whitespace (spaces, newlines) and parentheses to make the structure of the code easy to understand and the individual syntax pieces clear.\nUse consistent indentation to make the structure of the code easy to understand.\nProvide comments that give the goal of a given piece of code and why it does things, but don’t use comments to restate what the code does when it should be obvious from reading the code.\n\nProvide summaries for blocks of code.\nFor particularly complicated syntax, say what a given piece of code does."
  },
  {
    "objectID": "office_hours.html",
    "href": "office_hours.html",
    "title": "Office hours",
    "section": "",
    "text": "Chris (Evans 495 or Zoom (see Ed Discussion post for link))\n\nTBD\n\nJoão:\n\nTBD\nFridays during unused section time up until 3:30 pm (Evans 340)",
    "crumbs": [
      "Office Hours"
    ]
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "License",
    "section": "",
    "text": "This work is licensed under a Creative Commons Attribution 4.0 International License.",
    "crumbs": [
      "License"
    ]
  },
  {
    "objectID": "units/unit1-intro.html",
    "href": "units/unit1-intro.html",
    "title": "Introduction to UNIX, computers, and key tools",
    "section": "",
    "text": "PDF",
    "crumbs": [
      "Units",
      "Unit 1 (UNIX intro)"
    ]
  },
  {
    "objectID": "units/unit1-intro.html#summary-of-some-useful-editors",
    "href": "units/unit1-intro.html#summary-of-some-useful-editors",
    "title": "Introduction to UNIX, computers, and key tools",
    "section": "Summary of some useful editors",
    "text": "Summary of some useful editors\n\nvarious editors available on all operating systems:\n\ntraditional editors born in UNIX: emacs, vim\nsome newer editors: Atom, Sublime Text (Sublime is proprietary/not free)\n\nWindows-specific: WinEdt\nMac-specific: Aquamacs Emacs, TextMate, TextEdit\nRStudio provides a built-in editor for R code and Quarto/R Markdown files. One can actually edit and run Python code chunks quite nicely in RStudio. (Note: RStudio as a whole is an IDE (integrated development environment. The editor is just the editing window where you edit code (and Markdown) files.)\nVSCode has a powerful code editor that is customized to work with various languages, and it has a Quarto extension.\n\nAs you get started it’s ok to use a very simple text editor such as Notepad in Windows, but you should take the time in the next few weeks to try out more powerful editors such as one of those listed above. It will be well worth your time over the course of your graduate work and then your career.\nBe careful in Windows - file suffixes are often hidden.",
    "crumbs": [
      "Units",
      "Unit 1 (UNIX intro)"
    ]
  },
  {
    "objectID": "units/unit1-intro.html#optional-basic-emacs",
    "href": "units/unit1-intro.html#optional-basic-emacs",
    "title": "Introduction to UNIX, computers, and key tools",
    "section": "(Optional) Basic emacs",
    "text": "(Optional) Basic emacs\nEmacs is one option as an editor. I use Emacs a fair amount, so I’m including some tips here, but other editors listed above are just as good.\n\nEmacs has special modes for different types of files: Python code files, R code files, C code files, Latex files – it’s worth your time to figure out how to set this up on your machine for the kinds of files you often work on\n\nIf working with Python and R, one can start up a Python or R interpreter in an additional Emacs buffer and send code to that interpreter and see the results of running the code.\nFor working with R, ESS (emacs speaks statistics) mode is helpful. This is built into Aquamacs Emacs.\n\nTo open emacs in the terminal window rather than as a new window, which is handy when it’s too slow (or impossible) to pass (i.e., tunnel) the graphical emacs window through ssh: emacs -nw file.txt\n\n\n(Optional) Emacs keystroke sequence shortcuts (aka, key bindings).\n\nNote Several of these (Ctrl-a, Ctrl-e, Ctrl-k, Ctrl-y) work in the command line, interactive Python and R sessions, and other places as well.\n\n\n\n\n\n\n\n\nSequence\nResult\n\n\n\n\nCtrl-x,Ctrl-c\nClose the file\n\n\nCtrl-x,Ctrl-s\nSave the file\n\n\nCtrl-x,Ctrl-w\nSave with a new name\n\n\nCtrl-s\nSearch\n\n\nESC\nGet out of command buffer at bottom of screen\n\n\nCtrl-a\nGo to beginning of line\n\n\nCtrl-e\nGo to end of line\n\n\nCtrl-k\nDelete the rest of the line from cursor forward\n\n\nCtrl-space, then move to end of block\nHighlight a block of text\n\n\nCtrl-w\nRemove the highlighted block, putting it in the kill buffer\n\n\nCtrl-y (after using Ctrl-k or Ctrl-w)\nPaste from kill buffer (‘y’ is for ‘yank’)",
    "crumbs": [
      "Units",
      "Unit 1 (UNIX intro)"
    ]
  },
  {
    "objectID": "units/unit1-intro.html#optional-basic-vim",
    "href": "units/unit1-intro.html#optional-basic-vim",
    "title": "Introduction to UNIX, computers, and key tools",
    "section": "(Optional) Basic vim",
    "text": "(Optional) Basic vim\nvim is another option as an editor. Like emacs, it’s been around for a long time, and some of the other options above are probably more user friendly. However, it can be helpful to know how to do some basic things in vim.\nFor example, if you run git commit without the -m flag to add a message, you’ll be put in a vim editor window by default (you can also modify what editor git uses).\nvim has two modes: normal mode, which allows you to carry out various operations (such as navigation, saving files, moving and deleting lines) and insert mode, which allows you to actually insert text.\nTo get into insert mode from normal mode, type “i”. To get back to normal mode, press Esc.\nWhen in normal mode, you can type :w to save, :x to save and exit, and :q to exit. To search a document for a string (e.g., “python docstring”, type /python docstring and return/enter. Type Esc to get out of the search.",
    "crumbs": [
      "Units",
      "Unit 1 (UNIX intro)"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics 243: Introduction to Statistical Computing",
    "section": "",
    "text": "Ed\n\n  Gradescope\n\n  Tutorials\n\n  Lecture recordings\n\n  bCourses\n\n\nNo matching items",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#course-materials",
    "href": "index.html#course-materials",
    "title": "Statistics 243: Introduction to Statistical Computing",
    "section": "Course materials",
    "text": "Course materials\nSee the links above for the key resources for the course.\nMost course content (in particular unit notes, labs, and problem sets) are available through this website via the links in the left sidebar.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#questions-about-taking-the-class",
    "href": "index.html#questions-about-taking-the-class",
    "title": "Statistics 243: Introduction to Statistical Computing",
    "section": "Questions about taking the class",
    "text": "Questions about taking the class\nIf you would like to audit the class, enroll as a UC Berkeley undergraduate, or enroll as a concurrent enrollment student (i.e., for visiting students), or for some other reason are not enrolled, please fill out this survey as soon as possible. All those enrolled or wishing to take the class should have filled it out by Friday August 30 at noon.\nUndergraduates can only take the course with my permission and once all graduate students have an opportunity to register. I do not know if there will be space, but I recommend you attend the course as if you were enrolled until this is clear.\nConcurrent enrollment students (e.g., exchange students from other universities/countries) can take the course with my permission and once all UC Berkeley students have had an opportunity to register. I do not know if there will be space, but I recommend you attend the course as if you were enrolled until this is clear.\nPlease see the syllabus for the math and statistics background I expect, as well as the need to be familiar with Python or to get up to speed in Python during the first few weeks of the semester.\nThe first three weeks involve a lot of moving pieces, in part related to trying to get everyone up to speed with the bash shell and Python. Please use the schedule to keep on top of what you need to do.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Scheduling information",
    "section": "",
    "text": "Aug 29:\n           \n           Event 1 (Optional) Library Introduction to LaTeX with Overleaf session\n           \n                \n           \n                \n           \n        \n   \n\n        \n\n           \n           \n           Aug 30:\n           \n           Lab 0 (Optional) help session for software installation/setup and UNIX-style command line basics\n           \n                \n           \n                \n           \n        \n\n           \n           \n           Activity 1 Class Survey\n           \n                \n           \n                \n           \n        \n\n           \n           \n           Activity 2 Office hour time survey\n           \n                \n           \n                \n           \n        \n\n           \n           \n           Activity 3 Login to github.berkeley.edu with your CalNet credentials\n           \n                \n           \n                \n           \n        \n   \n   \n   \n  \n\n   \n\n   \n   \n   \n   \n   \n   \n\n        \n\n           \n           \n           Sep 4:\n           \n           Activity 4 Read first three sections of Unit 2 (sections before 'Webscraping')\n           \n                \n           \n                \n           \n        \n\n           \n           \n           Activity 5 (Optional) work through the UNIX basics tutorial and answer (for yourself) the questions at the end\n           \n                \n           \n                \n           \n        \n   \n\n        \n\n           \n           \n           Sep 6:\n           \n           Lab 1 Using Git and Quarto and problem set submission\n           \n                \n           \n                \n           \n        \n   \n   \n   \n  \n\n   \n\n   \n   \n   \n   \n   \n   \n\n        \n\n           \n           \n           Sep 11:\n           \n           Event 2 (Optional) Library Introduction to LaTeX with Overleaf session\n           \n                \n           \n                \n           \n        \n   \n   \n   \n  \n\n\nNo matching items",
    "crumbs": [
      "Schedule"
    ]
  },
  {
    "objectID": "schedule.html#schedule",
    "href": "schedule.html#schedule",
    "title": "Scheduling information",
    "section": "",
    "text": "Aug 29:\n           \n           Event 1 (Optional) Library Introduction to LaTeX with Overleaf session\n           \n                \n           \n                \n           \n        \n   \n\n        \n\n           \n           \n           Aug 30:\n           \n           Lab 0 (Optional) help session for software installation/setup and UNIX-style command line basics\n           \n                \n           \n                \n           \n        \n\n           \n           \n           Activity 1 Class Survey\n           \n                \n           \n                \n           \n        \n\n           \n           \n           Activity 2 Office hour time survey\n           \n                \n           \n                \n           \n        \n\n           \n           \n           Activity 3 Login to github.berkeley.edu with your CalNet credentials\n           \n                \n           \n                \n           \n        \n   \n   \n   \n  \n\n   \n\n   \n   \n   \n   \n   \n   \n\n        \n\n           \n           \n           Sep 4:\n           \n           Activity 4 Read first three sections of Unit 2 (sections before 'Webscraping')\n           \n                \n           \n                \n           \n        \n\n           \n           \n           Activity 5 (Optional) work through the UNIX basics tutorial and answer (for yourself) the questions at the end\n           \n                \n           \n                \n           \n        \n   \n\n        \n\n           \n           \n           Sep 6:\n           \n           Lab 1 Using Git and Quarto and problem set submission\n           \n                \n           \n                \n           \n        \n   \n   \n   \n  \n\n   \n\n   \n   \n   \n   \n   \n   \n\n        \n\n           \n           \n           Sep 11:\n           \n           Event 2 (Optional) Library Introduction to LaTeX with Overleaf session\n           \n                \n           \n                \n           \n        \n   \n   \n   \n  \n\n\nNo matching items",
    "crumbs": [
      "Schedule"
    ]
  },
  {
    "objectID": "schedule.html#quizzes",
    "href": "schedule.html#quizzes",
    "title": "Scheduling information",
    "section": "Quizzes",
    "text": "Quizzes\nQuizzes are in-person only.\n\nQuiz 1: TBD in class.\n\nReview session TBD in section.\n\nQuiz 2: TBD in class.\n\nReview session TBD in section.",
    "crumbs": [
      "Schedule"
    ]
  },
  {
    "objectID": "schedule.html#project",
    "href": "schedule.html#project",
    "title": "Scheduling information",
    "section": "Project",
    "text": "Project\nDue date: TBD.",
    "crumbs": [
      "Schedule"
    ]
  },
  {
    "objectID": "schedule.html#notes-on-assignments-and-activities",
    "href": "schedule.html#notes-on-assignments-and-activities",
    "title": "Scheduling information",
    "section": "Notes on assignments and activities",
    "text": "Notes on assignments and activities\n\nOptional library LaTeX session: I highly recommend (in particular if you are a Statistics graduate student) that you know how to create equations in LaTeX. Even if you develop your documents using Quarto, Jupyter notebooks, R Markdown, etc. rather than LaTeX-based documents, LaTeX math syntax is the common tool for writing math syntax that will render beautifully.\nOptional Lab 0 software/command line help session: (August 30 in lab room) Help session for installing software, accessing a UNIX-style command line, and basic command line usage (e.g., the UNIX basics tutorial). You can show up at any time (unlike all remaining labs). You should have software installed, be able to accesss the command line, and have started to become familiar with basic command line usage before class on Friday September 6.\nBash shell tutorial and exercises: (by TBD) Read through this tutorial on using the bash shell. You can skip the pages on Regular Expressions and Managing Processes. Work through the first 10 problems in the exercises and submit your answers via Gradescope. This is not a formal problem set, so you don’t need to worry about formatting nor about explaining/commenting your answers, nor do you need to put your answers in your GitHub class repository. In fact it’s even fine with me if you hand-write the answers and scan them to an electronic document. I just want to make sure you’ve worked through the tutorial. I’ll be doing demonstrations on using the bash shell in class starting on TBD, so that will be helpful as you work through the tutorial.\nLab 1: (September 6) First section/lab on using Git, setting up your GitHub repository for problem sets, and using Quarto to generate dynamic documents. Please come only to the section you are registered for given space limits in the room, unless you have talked with Chris and have his permission.\nRegular expression reading and exercises: (by TBD), read the regular expression material in the tutorial on using the bash shell. Then answer the regular expressions (regex) practice problems and submit your answers on Gradescope. This is not one of the graded problem sets but rather an assignment that will simply be noted as being completed or not.",
    "crumbs": [
      "Schedule"
    ]
  },
  {
    "objectID": "units/unit2-dataTech.html",
    "href": "units/unit2-dataTech.html",
    "title": "Data technologies, formats, and structures",
    "section": "",
    "text": "PDF\nReferences (see syllabus for links):\n(Optional) Videos:\nThere are four videos from 2020 in the bCourses Media Gallery that you can use for reference if you want to:\nNote that the videos were prepared for a version of the course that used R, so there are some differences from the content in the current version of the unit that reflect translating between R and Python. I’m not sure how helpful they’ll be, but they are available.",
    "crumbs": [
      "Units",
      "Unit 2 (Data technologies)"
    ]
  },
  {
    "objectID": "units/unit2-dataTech.html#text-and-binary-files",
    "href": "units/unit2-dataTech.html#text-and-binary-files",
    "title": "Data technologies, formats, and structures",
    "section": "Text and binary files",
    "text": "Text and binary files\nIn general, files can be divided into text files and binary files. In both cases, information is stored as a series of bits. Recall that a bit is a single value in base 2 (i.e., a 0 or a 1), while a byte is 8 bits.\nA text file is one in which the bits in the file encode individual characters. Note that the characters can include the digit characters 0-9, so one can include numbers in a text file by writing down the digits needed for the number of interest. Examples of text file formats include CSV, XML, HTML, and JSON.\nText files may be simple ASCII files (i.e., files encoded using ASCII) or files in other encodings such as UTF-8, both covered in Section 5. ASCII files have 8 bits (1 byte) per character and can represent 128 characters (the 52 lower and upper case letters in English, 10 digits, punctuation and a few other things – basically what you see on a standard US keyboard). UTF-8 files have between 1 and 4 bytes per character.\nSome text file formats, such as JSON or HTML, are not easily interpretable/manipulable on a line-by-line basis (unlike, e.g., CSV), so they are not as amenable to processing using shell commands.\nA binary file is one in which the bits in the file encode the information in a custom format and not simply individual characters. Binary formats are not (easily) human readable but can be more space-efficient and faster to work with (because it can allow random access into the data rather than requiring sequential reading). The meaning of the bytes in such files depends on the specific binary format being used and a program that uses the file needs to know how the format represents information. Examples of binary files include netCDF files, Python pickle files, R data (e.g., .Rda) files, , and compiled code files.\nNumbers in binary files are usually stored as 8 bytes per number. We’ll discuss this much more in Unit 8.",
    "crumbs": [
      "Units",
      "Unit 2 (Data technologies)"
    ]
  },
  {
    "objectID": "units/unit2-dataTech.html#common-file-types",
    "href": "units/unit2-dataTech.html#common-file-types",
    "title": "Data technologies, formats, and structures",
    "section": "Common file types",
    "text": "Common file types\nHere are some of the common file types, some of which are text formats and some of which are binary formats.\n\n‘Flat’ text files: data are often provided as simple text files. Often one has one record or observation per row and each column or field is a different variable or type of information about the record. Such files can either have a fixed number of characters in each field (fixed width format) or a special character (a delimiter) that separates the fields in each row. Common delimiters are tabs, commas, one or more spaces, and the pipe (|). Common file extensions are .txt and .csv. Metadata (information about the data) are often stored in a separate file. CSV files are quite common, but if you have files where the data contain commas, other delimiters might be preferable. Text can be put in quotes in CSV files, and this can allow use of commas within the data. This is difficult to deal with from the command line, but read_table() in Pandas handles this situation.\n\nOne occasionally tricky difficulty is as follows. If you have a text file created in Windows, the line endings are coded differently than in UNIX. Windows uses a newline (the ASCII character \\n) and a carriage return (the ASCII character \\r) whereas UNIX uses onlyl a newline in UNIX). There are UNIX utilities (fromdos in Ubuntu, including the SCF Linux machines and dos2unix in other Linux distributions) that can do the necessary conversion. If you see \\^M at the end of the lines in a file, that’s the tool you need. Alternatively, if you open a UNIX file in Windows, it may treat all the lines as a single line. You can fix this with todos or unix2dos.\n\nIn some contexts, such as textual data and bioinformatics data, the data may be in a text file with one piece of information per row, but without meaningful columns/fields.\nData may also be in text files in formats designed for data interchange between various languages, in particular XML or JSON. These formats are “self-describing”; namely the metadata is part of the file. The lxml and json packages are useful for reading and writing from these formats. More in Section 4.\nYou may be scraping information on the web, so dealing with text files in various formats, including HTML. The requests and BeautifulSoup packages are useful for reading HTML.\nIn scientific contexts, netCDF (.nc) (and the related HDF5) are popular format for gridded data that allows for highly-efficient storage and contains the metadata within the file. The basic structure of a netCDF file is that each variable is an array with multiple dimensions (e.g., latitude, longitude, and time), and one can also extract the values of and metadata about each dimension. The netCDF4 package in Python nicely handles working with netCDF files.\nData may already be in a database or in the data storage format of another statistical package (Stata, SAS, SPSS, etc.). The Pandas package in Python has capabilities for importing Stata (read_stata), SPSS (read_spss), and SAS (read_sas) files, among others.\nFor Excel, there are capabilities to read an Excel file (see the read_excel function in Pandas), but you can also just go into Excel and export as a CSV file or the like and then read that into Python. In general, it’s best not to pass around data files as Excel or other spreadsheet format files because (1) Excel is proprietary, so someone may not have Excel and the format is subject to change, (2) Excel imposes limits on the number of rows, (3) one can easily manipulate text files such as CSV using UNIX tools, but this is not possible with an Excel file, (4) Excel files often have more than one sheet, graphs, macros, etc., so they’re not a data storage format per se.\nPython can easily interact with databases (SQLite, PostgreSQL, MySQL, Oracle, etc.), querying the database using SQL and returning results to Python. More in the big data unit and in the large datasets tutorial mentioned above.",
    "crumbs": [
      "Units",
      "Unit 2 (Data technologies)"
    ]
  },
  {
    "objectID": "units/unit2-dataTech.html#csv-vs.-specialized-formats-such-as-parquet",
    "href": "units/unit2-dataTech.html#csv-vs.-specialized-formats-such-as-parquet",
    "title": "Data technologies, formats, and structures",
    "section": "CSV vs. specialized formats such as Parquet",
    "text": "CSV vs. specialized formats such as Parquet\nCSV is a common format (particularly in some disciplines/contexts) and has the advantages of being simple to understand, human readable, and readily manipulable by line-based processing tools such as shell commands. However, it has various disadvantages:\n\nstorage is by row, which will often mix values of different types;\nextra space is taken up by explicitly storing commas and newlines; and\none must search through the document to find a given row or value – e.g., to find the 10th row, we must search for the 9th newline and then read until the 10th newline.\n\nA popular file format that has some advantages over plain text formats such as CSV is Parquet. The storage is by column (actually in chunks of columns). This works well with how datasets are often structured in that a given field/variable will generally have values of all the same type and there may be many repeated values, so there are opportunities for efficient storage including compression. Storage by column also allows retrieval only of the columns that a user needs. As a result data stored in the Parquet format often takes up much less space than stored as CSV and can be queried much faster. Also note that data stored in Parquet will often be stored as multiple files.\nHere’s a brief exploration using a data file not in the class repository.\n\nimport time\n\n## Read from CSV\nt0 = time.time()\ndata_from_csv = pd.read_csv(os.path.join('..', 'data', 'airline.csv'))\nprint(time.time() - t0)\n\n1.0871880054473877\n\n\n\n## Write out Parquet-formatted data\ndata_from_csv.to_parquet(os.path.join('..', 'data', 'airline.parquet'))\n\n## Read from Parquet\nt0 = time.time()\ndata_from_parquet = pd.read_parquet(os.path.join(\n                                    '..', 'data', 'airline.parquet'))\nprint(time.time() - t0)\n\n0.13507437705993652\n\n\nThe CSV file is 51 MB while the Parquet file is 8 MB.\n\nimport subprocess\nsubprocess.run([\"ls\", \"-l\", os.path.join(\"..\", \"data\", \"airline.csv\")])\nsubprocess.run([\"ls\", \"-l\", os.path.join(\"..\", \"data\", \"airline.parquet\")])\n\n-rw-r--r-- 1 paciorek scfstaff 51480244 Aug 27 11:35 ../data/airline.csv\n\n\nCompletedProcess(args=['ls', '-l', '../data/airline.csv'], returncode=0)\n\n\n-rw-r--r-- 1 paciorek scfstaff 8153160 Aug 27 12:31 ../data/airline.parquet\n\n\nCompletedProcess(args=['ls', '-l', '../data/airline.parquet'], returncode=0)",
    "crumbs": [
      "Units",
      "Unit 2 (Data technologies)"
    ]
  },
  {
    "objectID": "units/unit2-dataTech.html#core-python-functions",
    "href": "units/unit2-dataTech.html#core-python-functions",
    "title": "Data technologies, formats, and structures",
    "section": "Core Python functions",
    "text": "Core Python functions\nThe read_table and read_csv functions in the Pandas package are commonly used for reading in data. They read in delimited files (CSV specifically in the latter case). The key arguments are the delimiter (the sep argument) and whether the file contains a header, a line with the variable names. We can use read_fwf() to read from a fixed width text file into a data frame.\nThe most difficult part of reading in such files can be dealing with how Pandas determines the types of the fields that are read in. While Pandas will try to determine the types automatically, it can be safer (and faster) to tell Pandas what the types are, using the dtype argument to read_table().\nLet’s work through a couple examples. Before we do that, let’s look at the arguments to read_table. Note that sep='' can use regular expressions (which would be helpful if you want to separate on any amount of white space, as one example).\n\ndat = pd.read_table(os.path.join('..', 'data', 'RTADataSub.csv'),\n                    sep = ',', header = None)\ndat.dtypes.head()   # 'object' is string or mixed type\ndat.loc[0,1]     \ntype(dat.loc[0,1]) # string!\n## Whoops, there is an 'x', presumably indicating missingness:\ndat.loc[:,1].unique()\n\n0    object\n1    object\n2    object\n3    object\n4    object\ndtype: object\n\n\n'2336'\n\n\nstr\n\n\narray(['2336', '2124', '1830', '1833', '1600', '1578', '1187', '1005',\n       '918', '865', '871', '860', '883', '897', '898', '893', '913',\n       '870', '962', '880', '875', '884', '894', '836', '848', '885',\n       '851', '900', '861', '866', '867', '829', '853', '920', '877',\n       '908', '855', '845', '859', '856', '825', '828', '854', '847',\n       '840', '873', '822', '818', '838', '815', '813', '816', '849',\n       '802', '805', '792', '823', '808', '798', '800', '842', '809',\n       '807', '826', '810', '801', '794', '771', '796', '790', '787',\n       '775', '751', '783', '811', '768', '779', '795', '770', '821',\n       '830', '767', '772', '791', '781', '773', '777', '814', '778',\n       '782', '837', '759', '846', '797', '835', '832', '793', '803',\n       '834', '785', '831', '820', '812', '824', '728', '760', '762',\n       '753', '758', '764', '741', '709', '735', '749', '752', '761',\n       '750', '776', '766', '789', '763', '864', '858', '869', '886',\n       '844', '863', '916', '890', '872', '907', '926', '935', '933',\n       '906', '905', '912', '972', '996', '1009', '961', '952', '981',\n       '917', '1011', '1071', '1920', '3245', '3805', '3926', '3284',\n       '2700', '2347', '2078', '2935', '3040', '1860', '1437', '1512',\n       '1720', '1493', '1026', '928', '874', '833', '850', nan, 'x'],\n      dtype=object)\n\n\n\n## Let's treat 'x' as a missing value indicator.\ndat2 = pd.read_table(os.path.join('..', 'data', 'RTADataSub.csv'),\n                     sep = ',', header = None, na_values = 'x')\ndat2.dtypes.head()\ndat2.loc[:,1].unique()\n\n0     object\n1    float64\n2    float64\n3    float64\n4    float64\ndtype: object\n\n\narray([2336., 2124., 1830., 1833., 1600., 1578., 1187., 1005.,  918.,\n        865.,  871.,  860.,  883.,  897.,  898.,  893.,  913.,  870.,\n        962.,  880.,  875.,  884.,  894.,  836.,  848.,  885.,  851.,\n        900.,  861.,  866.,  867.,  829.,  853.,  920.,  877.,  908.,\n        855.,  845.,  859.,  856.,  825.,  828.,  854.,  847.,  840.,\n        873.,  822.,  818.,  838.,  815.,  813.,  816.,  849.,  802.,\n        805.,  792.,  823.,  808.,  798.,  800.,  842.,  809.,  807.,\n        826.,  810.,  801.,  794.,  771.,  796.,  790.,  787.,  775.,\n        751.,  783.,  811.,  768.,  779.,  795.,  770.,  821.,  830.,\n        767.,  772.,  791.,  781.,  773.,  777.,  814.,  778.,  782.,\n        837.,  759.,  846.,  797.,  835.,  832.,  793.,  803.,  834.,\n        785.,  831.,  820.,  812.,  824.,  728.,  760.,  762.,  753.,\n        758.,  764.,  741.,  709.,  735.,  749.,  752.,  761.,  750.,\n        776.,  766.,  789.,  763.,  864.,  858.,  869.,  886.,  844.,\n        863.,  916.,  890.,  872.,  907.,  926.,  935.,  933.,  906.,\n        905.,  912.,  972.,  996., 1009.,  961.,  952.,  981.,  917.,\n       1011., 1071., 1920., 3245., 3805., 3926., 3284., 2700., 2347.,\n       2078., 2935., 3040., 1860., 1437., 1512., 1720., 1493., 1026.,\n        928.,  874.,  833.,  850.,   nan])\n\n\nUsing dtype is a good way to control how data are read in.\n\ndat = pd.read_table(os.path.join('..', 'data', 'hivSequ.csv'),\n                  sep = ',', header = 0,\n                  dtype = {\n                  'PatientID': int,\n                  'Resp': int,\n                  'PR Seq': str,\n                  'RT Seq': str,\n                  'VL-t0': float,\n                  'CD4-t0': int})\ndat.dtypes\ndat.loc[0,'PR Seq']\n\nPatientID      int64\nResp           int64\nPR Seq        object\nRT Seq        object\nVL-t0        float64\nCD4-t0         int64\ndtype: object\n\n\n'CCTCAAATCACTCTTTGGCAACGACCCCTCGTCCCAATAAGGATAGGGGGGCAACTAAAGGAAGCYCTATTAGATACAGGAGCAGATGATACAGTATTAGAAGACATGGAGTTGCCAGGAAGATGGAAACCAAAAATGATAGGGGGAATTGGAGGTTTTATCAAAGTAARACAGTATGATCAGRTACCCATAGAAATCTATGGACATAAAGCTGTAGGTACAGTATTAATAGGACCTACACCTGTCAACATAATTGGAAGAAATCTGTTGACTCAGCTTGGTTGCACTTTAAATTTY'\n\n\nNote that you can avoid reading in one or more columns by using the usecols argument. Also, specifying the dtype argument explicitly should make for faster file reading.\nIf possible, it’s a good idea to look through the input file in the shell or in an editor before reading into Python to catch such issues in advance. Using the UNIX command less on RTADataSub.csv would have revealed these various issues, but note that RTADataSub.csv is a 1000-line subset of a much larger file of data available from the kaggle.com website. So more sophisticated use of UNIX utilities (as we will see in Unit 3) is often useful before trying to read something into a program.\nIf the file is not nicely arranged by field (e.g., if it has ragged lines), we’ll need to do some more work. We can read each line as a separate string, after which we can process the lines using text manipulation. Here’s an example from some US meteorological data where I know from metadata (not provided here) that the 4-11th values are an identifier, the 17-20th are the year, the 22-23rd the month, etc.\n\nfile_path = os.path.join('..', 'data', 'precip.txt')\nwith open(file_path, 'r') as file:\n     lines = file.readlines()\n\nid = [line[3:11] for line in lines]\nyear = [int(line[17:21]) for line in lines]\nmonth = [int(line[21:23]) for line in lines]\nnvalues = [int(line[27:30]) for line in lines]\nyear[0:5]\n\n[2010, 2010, 2010, 2010, 2010]\n\n\nActually, that file, precip.txt, is in a fixed-width format (i.e., every element in a given column has the exact same number of characters),so reading in using pandas.read_fwf() would be a good strategy.",
    "crumbs": [
      "Units",
      "Unit 2 (Data technologies)"
    ]
  },
  {
    "objectID": "units/unit2-dataTech.html#connections-and-streaming",
    "href": "units/unit2-dataTech.html#connections-and-streaming",
    "title": "Data technologies, formats, and structures",
    "section": "Connections and streaming",
    "text": "Connections and streaming\nPython allows you to read in not just from a file but from a more general construct called a connection. This can include reading in text from the output of running a shell command and from unzipping a file on the fly.\nHere are some examples of connections:\n\nimport gzip\nwith gzip.open('dat.csv.gz', 'r') as file:\n     lines = file.readlines()\n\nimport zipfile\nwith zipfile.ZipFile('dat.zip', 'r') as archive:\n     with archive.open('data.txt', 'r') as file:\n          lines = file.readlines()\n\nimport subprocess\ncommand = \"ls -al\"\noutput = subprocess.check_output(command, shell = True)\n# `output` is a sequence of bytes.\nwith io.BytesIO(output) as stream:  # Create a file-like object.\n    content = stream.readlines()\n\ndf = pd.read_csv(\"https://download.bls.gov/pub/time.series/cu/cu.item\", sep=\"\\t\")\n\nIf a file is large, we may want to read it in in chunks (of lines), do some computations to reduce the size of things, and iterate. This is referred to as online processing, streaming, or chunking, and can be done using Pandas (among other tools).\n\nfile_path = os.path.join('..', 'data', 'RTADataSub.csv')\nchunksize = 50 # Obviously this would be much larger in any real application.\n\nwith pd.read_csv(file_path, chunksize = chunksize) as reader:\n     for chunk in reader:\n         # manipulate the lines and store the key stuff\n         print(f'Read {len(chunk)} rows.')\n\nMore details on sequential (on-line) processing of large files can be found in the tutorial on large datasets mentioned in the reference list above.\nOne cool trick that can come in handy is to ‘read’ from a string as if it were a text file. Here’s an example:\n\nfile_path = os.path.join('..', 'data', 'precip.txt')\nwith open(file_path, 'r') as file:\n     text = file.read()\n\nstringIOtext = io.StringIO(text)\ndf = pd.read_fwf(stringIOtext, header = None, widths = [3,8,4,2,4,2])\n\nWe can create connections for writing output too. Just make sure to open the connection first.",
    "crumbs": [
      "Units",
      "Unit 2 (Data technologies)"
    ]
  },
  {
    "objectID": "units/unit2-dataTech.html#file-paths",
    "href": "units/unit2-dataTech.html#file-paths",
    "title": "Data technologies, formats, and structures",
    "section": "File paths",
    "text": "File paths\nA few notes on file paths, related to ideas of reproducibility.\n\nIn general, you don’t want to hard-code absolute paths into your code files because those absolute paths won’t be available on the machines of anyone you share the code with. Instead, use paths relative to the directory the code file is in, or relative to a baseline directory for the project, e.g.:\n\n\ndat = pd.read_csv('../data/cpds.csv')\n\nUsing UNIX style directory separators will work in Windows, Mac or Linux, but using Windows style separators is not portable across operating systems.\n\n## good: will work on Windows\ndat = pd.read_csv('../data/cpds.csv')\n## bad: won't work on Mac or Linux\ndat = pd.read_csv('..\\data\\cpds.csv')  \n\nEven better, use os.path.join so that paths are constructed specifically for the operating system the user is using:\n\n\n## good: operating-system independent\ndat = pd.read_csv(os.path.join('..', 'data', 'cpds.csv'))",
    "crumbs": [
      "Units",
      "Unit 2 (Data technologies)"
    ]
  },
  {
    "objectID": "units/unit2-dataTech.html#reading-data-quickly-arrow-and-polars",
    "href": "units/unit2-dataTech.html#reading-data-quickly-arrow-and-polars",
    "title": "Data technologies, formats, and structures",
    "section": "Reading data quickly: Arrow and Polars",
    "text": "Reading data quickly: Arrow and Polars\nApache Arrow provides efficient data structures for working with data in memory, usable in Python via the PyArrow package. Data are stored by column, with values in a column stored sequentially and in such a way that one can access a specific value without reading the other values in the column (O(1) lookup). Arrow is designed to read data from various file formats, including Parquet, native Arrow format, and text files. In general Arrow will only read data from disk as needed, avoiding keeping the entire dataset in memory.\nOther options for avoiding reading all your data into memory include the Dask package and using numpy.load with the mmap_mode argument.\npolars is designed to be a faster alternative to Pandas for working with data in-memory.\n\nimport polars\nimport time\nt0 = time.time()\ndat = pd.read_csv(os.path.join('..', 'data', 'airline.csv'))\nt1 = time.time()\ndat2 = polars.read_csv(os.path.join('..', 'data', 'airline.csv'), null_values = ['NA'])\nt2 = time.time()\nprint(f\"Timing for Pandas: {t1-t0}.\")\nprint(f\"Timing for Polars: {t2-t1}.\")\n\nTiming for Pandas: 1.0616731643676758.\nTiming for Polars: 0.24181413650512695.",
    "crumbs": [
      "Units",
      "Unit 2 (Data technologies)"
    ]
  },
  {
    "objectID": "units/unit2-dataTech.html#writing-output-to-files",
    "href": "units/unit2-dataTech.html#writing-output-to-files",
    "title": "Data technologies, formats, and structures",
    "section": "Writing output to files",
    "text": "Writing output to files\nFunctions for text output are generally analogous to those for input.\n\nfile_path = os.path.join('/tmp', 'tmp.txt')\nwith open(file_path, 'w') as file:\n     file.writelines(lines)\n\nWe can also use file.write() to write individual strings.\nIn Pandas, we can use DataFrame.to_csv and DataFrame.to_parquet.\nWe can use the json.dump function to output appropriate data objects (e.g., dictionaries or possibly lists) as JSON. One use of JSON as output from Python would be to ‘serialize’ the information in an Python object such that it could be read into another program.\nAnd of course you can always save to a Pickle data file (a binary file format) using pickle.dump() and pickle.load() from the pickle package. Happily this is platform-independent so can be used to transfer Python objects between different OS.",
    "crumbs": [
      "Units",
      "Unit 2 (Data technologies)"
    ]
  },
  {
    "objectID": "units/unit2-dataTech.html#formatting-output",
    "href": "units/unit2-dataTech.html#formatting-output",
    "title": "Data technologies, formats, and structures",
    "section": "Formatting output",
    "text": "Formatting output\nWe can use string formatting to control how output is printed to the screen.\nThe mini-language involved in the format specification can get fairly involved, but a few basic pieces of syntax can do most of what one generally needs to do.\nWe can format numbers to chosen number of digits and decimal places and handle alignment, using the format method of the string class.\nFor example:\n\n'{:&gt;10}'.format(3.5)    # right-aligned, using 10 characters\n'{:.10f}'.format(1/3)   # force 10 decimal places\n'{:15.10f}'.format(1/3) # force 15 characters, with 10 decimal places\nformat(1/3, '15.10f') # alternative using a function\n\n'       3.5'\n\n\n'0.3333333333'\n\n\n'   0.3333333333'\n\n\n'   0.3333333333'\n\n\nWe can also “interpolate” variables into strings.\n\n\"The number pi is {}.\".format(np.pi)\n\"The number pi is {:.5f}.\".format(np.pi)\n\"The number pi is {:.12f}.\".format(np.pi)\n\n'The number pi is 3.141592653589793.'\n\n\n'The number pi is 3.14159.'\n\n\n'The number pi is 3.141592653590.'\n\n\n\nval1 = 1.5\nval2 = 2.5\n# As of Python 3.6, put the variable names in directly.\nprint(f\"Let's add {val1} and {val2}.\")  \nnum1 = 1/3\nprint(\"Let's add the %s numbers %.5f and %15.7f.\"\n       %('floating point', num1 ,32+1/7))\n\nLet's add 1.5 and 2.5.\nLet's add the floating point numbers 0.33333 and      32.1428571.\n\n\nOr to insert into a file:\n\nfile_path = os.path.join('/tmp', 'tmp.txt')\nwith open(file_path, 'a') as file:\n     file.write(\"Let's add the %s numbers %.5f and %15.7f.\"\n                %('floating point', num1 ,32+1/7))\n\nround is another option, but it’s often better to directly control the printing format.",
    "crumbs": [
      "Units",
      "Unit 2 (Data technologies)"
    ]
  },
  {
    "objectID": "units/unit2-dataTech.html#reading-html",
    "href": "units/unit2-dataTech.html#reading-html",
    "title": "Data technologies, formats, and structures",
    "section": "Reading HTML",
    "text": "Reading HTML\nHTML (Hypertext Markup Language) is the standard markup language used for displaying content in a web browser. In simple webpages (ignoring the more complicated pages that involve Javascript), what you see in your browser is simply a rendering (by the browser) of a text file containing HTML.\nHowever, instead of rendering the HTML in a browser, we might want to use code to extract information from the HTML.\nLet’s see a brief example of reading in HTML tables.\nNote that before doing any coding, it can be helpful to look at the raw HTML source code for a given page. We can explore the underlying HTML source in advance of writing our code by looking at the page source directly in the browser (e.g., in Firefox under the 3-lines (hamburger) “open menu” symbol, see Web Developer (or More Tools) -&gt; Page Source and in Chrome View -&gt; Developer -&gt; View Source), or by downloading the webpage and looking at it in an editor, although in some cases (such as the nytimes.com case), what we might see is a lot of JavaScript.\nOne lesson here is not to write a lot of your own code to do something that someone else has probably already written a package for. We’ll use the BeautifulSoup4 package.\n\nimport io\nimport requests\nfrom bs4 import BeautifulSoup as bs\n\nURL = \"https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population\"\nresponse = requests.get(URL)\nhtml = response.content\n\n# Create a BeautifulSoup object to parse the HTML\nsoup = bs(html, 'html.parser')\n\nhtml_tables = soup.find_all('table')\n\n## Pandas `read_html` doesn't want `str` input directly.\npd_tables = [pd.read_html(io.StringIO(str(tbl)))[0] for tbl in html_tables]\n\n[x.shape[0] for x in pd_tables]\n\npd_tables[0].head()\n\n[241, 13, 1]\n\n\n\n\n\n\n\n\n\nUnnamed: 0\nLocation\nPopulation\n% of world\nDate\nSource (official or from the United Nations)\nNotes\n\n\n\n\n0\n–\nWorld\n8126685000\n100%\n27 Aug 2024\nUN projection[3]\nNaN\n\n\n1\n1/2 [b]\nChina\n1409670000\n17.3%\n31 Dec 2023\nOfficial estimate[5]\n[c]\n\n\n2\n1/2 [b]\nIndia\n1404910000\n17.3%\n1 Jul 2024\nOfficial projection[6]\n[d]\n\n\n3\n3\nUnited States\n335893238\n4.1%\n1 Jan 2024\nOfficial estimate[7]\n[e]\n\n\n4\n4\nIndonesia\n281603800\n3.5%\n1 Jul 2024\nNational annual projection[8]\nNaN\n\n\n\n\n\n\n\nBeautiful Soup works by reading in the HTML as text and then parsing it to build up a tree containing the HTML elements. Then one can search by HTML tag or attribute for information you want using find_all.\nAs another example, it’s often useful to be able to extract the hyperlinks in an HTML document.\n\nURL = \"http://www1.ncdc.noaa.gov/pub/data/ghcn/daily/by_year\"\nresponse = requests.get(URL)\nsoup = bs(response.content, 'html.parser')\n\n## Approach 1: search for HTML 'a' tags.\na_elements = soup.find_all('a')\nlinks1 = [x.get('href') for x in a_elements]\n## Approach 2: search for 'a' elements with 'href' attribute\nhref_elements = soup.find_all('a', href = True)\nlinks2 = [x.get('href') for x in href_elements]\n## In either case, then use `get` to retrieve the `href` attribute value.\n\nlinks2[0:9]\n# help(bs.find_all)\n\n['?C=N;O=D',\n '?C=M;O=A',\n '?C=S;O=A',\n '?C=D;O=A',\n '/pub/data/ghcn/daily/',\n '1750.csv.gz',\n '1763.csv.gz',\n '1764.csv.gz',\n '1765.csv.gz']\n\n\nThe kwargs keyword arguments to find and find_all allow one to search for elements with particular characteristics, such as having a particular attribute (seen above) or having an attribute have a particular value (e.g., picking out an element with a particular id).\nHere’s another example of extracting specific components of information from a webpage (results not shown, since headlines will vary from day to day). We’ll use get_text to retrieve the element’s value.\n\nURL = \"https://www.nytimes.com\"\nresponseNYT = requests.get(URL)\nsoupNYT = bs(responseNYT.content, 'html.parser')\nh2_elements = soupNYT.find_all(\"h2\")\nheadlines2 = [x.get_text() for x in h2_elements]\nh3_elements = soupNYT.find_all(\"h3\")\nheadlines3 = [x.get_text() for x in h3_elements]\n\nMore generally, we may want to read an HTML document, parse it into its components (i.e., the HTML elements), and navigate through the tree structure of the HTML.\nWe can use CSS selectors with the select method for more powerful extraction capabilities. Going back to the climate data, let’s extract all the th elements nested within tr elements:\n\nsoup.select(\"tr th\")\n\n[&lt;th&gt;&lt;a href=\"?C=N;O=D\"&gt;Name&lt;/a&gt;&lt;/th&gt;,\n &lt;th&gt;&lt;a href=\"?C=M;O=A\"&gt;Last modified&lt;/a&gt;&lt;/th&gt;,\n &lt;th&gt;&lt;a href=\"?C=S;O=A\"&gt;Size&lt;/a&gt;&lt;/th&gt;,\n &lt;th&gt;&lt;a href=\"?C=D;O=A\"&gt;Description&lt;/a&gt;&lt;/th&gt;,\n &lt;th colspan=\"4\"&gt;&lt;hr/&gt;&lt;/th&gt;,\n &lt;th colspan=\"4\"&gt;&lt;hr/&gt;&lt;/th&gt;]\n\n\nOr we could extract the a elements whose parents are th elements:\n\nsoup.select(\"th &gt; a\")\n\n[&lt;a href=\"?C=N;O=D\"&gt;Name&lt;/a&gt;,\n &lt;a href=\"?C=M;O=A\"&gt;Last modified&lt;/a&gt;,\n &lt;a href=\"?C=S;O=A\"&gt;Size&lt;/a&gt;,\n &lt;a href=\"?C=D;O=A\"&gt;Description&lt;/a&gt;]\n\n\nNext let’s use the XPath language to specify elements rather than CSS selectors. XPath can also be used for navigating through XML documents.\n\nimport lxml.html\n\n# Convert the BeautifulSoup object to a lxml object\nlxml_doc = lxml.html.fromstring(str(soup))\n\n# Use XPath to select elements\na_elements = lxml_doc.xpath('//a[@href]')\nlinks = [x.get('href') for x in a_elements]\nlinks[0:9]\n\n['?C=N;O=D',\n '?C=M;O=A',\n '?C=S;O=A',\n '?C=D;O=A',\n '/pub/data/ghcn/daily/',\n '1750.csv.gz',\n '1763.csv.gz',\n '1764.csv.gz',\n '1765.csv.gz']",
    "crumbs": [
      "Units",
      "Unit 2 (Data technologies)"
    ]
  },
  {
    "objectID": "units/unit2-dataTech.html#xml-json-and-yaml",
    "href": "units/unit2-dataTech.html#xml-json-and-yaml",
    "title": "Data technologies, formats, and structures",
    "section": "XML, JSON, and YAML",
    "text": "XML, JSON, and YAML\nXML, JSON, and YAML are three common file formats for storing data. All of them allow for key-value pairs and arrays/lists of unnamed elements and for hierarchical structure.\nTo read them into Python (or other languages), we want to use a package that understands the file format and can read the data into appropriate Python data structures. Usually one ends up with a set of nested (because of the hierarchical structure) lists and dictionaries.\n\nXML\nXML is a markup language used to store data in self-describing (no metadata needed) format, often with a hierarchical structure. It consists of sets of elements (also known as nodes because they generally occur in a hierarchical structure and therefore have parents, children, etc.) with tags that identify/name the elements, with some similarity to HTML. Some examples of the use of XML include serving as the underlying format for Microsoft Office and Google Docs documents and for the KML language used for spatial information in Google Earth.\nHere’s a brief example. The book with id attribute bk101 is an element; the author of the book is also an element that is a child element of the book. The id attribute allows us to uniquely identify the element.\n    &lt;?xml version=\"1.0\"?&gt;\n    &lt;catalog&gt;\n       &lt;book id=\"bk101\"&gt;\n          &lt;author&gt;Gambardella, Matthew&lt;/author&gt;\n          &lt;title&gt;XML Developer's Guide&lt;/title&gt;\n          &lt;genre&gt;Computer&lt;/genre&gt;\n          &lt;price&gt;44.95&lt;/price&gt;\n          &lt;publish_date&gt;2000-10-01&lt;/publish_date&gt;\n          &lt;description&gt;An in-depth look at creating applications with XML.&lt;/description&gt;\n       &lt;/book&gt;\n       &lt;book id=\"bk102\"&gt;\n          &lt;author&gt;Ralls, Kim&lt;/author&gt;\n          &lt;title&gt;Midnight Rain&lt;/title&gt;\n          &lt;genre&gt;Fantasy&lt;/genre&gt;\n          &lt;price&gt;5.95&lt;/price&gt;\n          &lt;publish_date&gt;2000-12-16&lt;/publish_date&gt;\n         &lt;description&gt;A former architect battles corporate zombies, an evil sorceress, and her own childhood to become queen of the world.&lt;/description&gt;\n       &lt;/book&gt;\n    &lt;/catalog&gt;\nWe can read XML documents into Python using various packages, including lxml and then manipulate the resulting structured data object. Here’s an example of working with lending data from the Kiva lending non-profit. You can see the XML format in a browser at http://api.kivaws.org/v1/loans/newest.xml.\nXML documents have a tree structure with information at nodes. As above with HTML, one can use the XPath language for navigating the tree and finding and extracting information from the node(s) of interest.\nHere is some example code for extracting loan info from the Kiva data. We’ll first show the ‘brute force’ approach of working with the data as a list and then the better approach of using XPath.\n\nimport xmltodict\n\nURL = \"https://api.kivaws.org/v1/loans/newest.xml\"\nresponse = requests.get(URL)\ndata = xmltodict.parse(response.content)\ndata.keys()\ndata['response'].keys()\ndata['response']['loans'].keys()\nlen(data['response']['loans']['loan'])\ndata['response']['loans']['loan'][2]\ndata['response']['loans']['loan'][2]['activity']\n\ndict_keys(['response'])\n\n\ndict_keys(['paging', 'loans'])\n\n\ndict_keys(['@type', 'loan'])\n\n\n20\n\n\n{'id': '2834862',\n 'name': 'Claudia',\n 'description': {'languages': {'@type': 'list', 'language': 'en'}},\n 'status': 'fundraising',\n 'funded_amount': '0',\n 'basket_amount': '0',\n 'image': {'id': '5641122', 'template_id': '1'},\n 'activity': 'Tailoring',\n 'sector': 'Services',\n 'themes': {'@type': 'list',\n  'theme': ['Underfunded Areas',\n   'Youth',\n   'Job Creation',\n   'Growing Businesses']},\n 'use': 'to purchase an overlock machine that will optimize the production of her workshop.',\n 'location': {'country_code': 'CO',\n  'country': 'Colombia',\n  'town': 'CALI - VALLE DEL CAUCA',\n  'geo': {'level': 'town', 'pairs': '3.451647 -76.531985', 'type': 'point'}},\n 'partner_id': '621',\n 'posted_date': '2024-08-27T19:20:29Z',\n 'planned_expiration_date': '2024-10-01T19:20:29Z',\n 'loan_amount': '325',\n 'borrower_count': '1',\n 'lender_count': '0',\n 'bonus_credit_eligibility': '1',\n 'tags': {'@type': 'list',\n  'tag': [{'name': '#Woman-Owned Business', 'id': '6'},\n   {'name': '#Single', 'id': '14'},\n   {'name': '#Single Parent', 'id': '17'},\n   {'name': '#Fabrics', 'id': '26'},\n   {'name': '#Biz Durable Asset', 'id': '35'}]}}\n\n\n'Tailoring'\n\n\n\nfrom lxml import etree\ndoc = etree.fromstring(response.content)\n\nloans = doc.xpath(\"//loan\")\n[loan.xpath(\"activity/text()\") for loan in loans]\n\n## suppose we only want the country locations of the loans (using XPath)\n[loan.xpath(\"location/country/text()\") for loan in loans]\n## or extract the geographic coordinates\n[loan.xpath(\"location/geo/pairs/text()\") for loan in loans]\n\n[['Agriculture'],\n ['Clothing Sales'],\n ['Tailoring'],\n ['Agriculture'],\n ['Sewing'],\n ['General Store'],\n ['Agriculture'],\n ['Agriculture'],\n ['Restaurant'],\n ['Vehicle Repairs'],\n ['Pigs'],\n ['Farming'],\n ['Pigs'],\n ['Pigs'],\n ['Farming'],\n ['Fruits & Vegetables'],\n ['Farming'],\n ['Pigs'],\n ['Beauty Salon'],\n ['Transportation']]\n\n\n[['Colombia'],\n ['Colombia'],\n ['Colombia'],\n ['Colombia'],\n ['Colombia'],\n ['Colombia'],\n ['Colombia'],\n ['Colombia'],\n ['Colombia'],\n ['Kenya'],\n ['Madagascar'],\n ['Madagascar'],\n ['Madagascar'],\n ['Madagascar'],\n ['Madagascar'],\n ['El Salvador'],\n ['Kenya'],\n ['Colombia'],\n ['Colombia'],\n ['Colombia']]\n\n\n[['2.682782 -76.571941'],\n ['2.444814 -76.61474'],\n ['3.451647 -76.531985'],\n ['2.025561 -75.758955'],\n ['1.205884 -77.285787'],\n ['2.934484 -75.2809'],\n ['2.025561 -75.758955'],\n ['1.993097 -76.043614'],\n ['1.205884 -77.285787'],\n ['0.007442 37.07223'],\n ['-19.962173 47.042591'],\n ['-19.829593 47.054448'],\n ['-19.829593 47.054448'],\n ['-19.829593 47.054448'],\n ['-19.829593 47.054448'],\n ['13.325801 -88.561363'],\n ['-0.777114 34.945839'],\n ['2.070635 -77.051604'],\n ['1.856331 -76.046161'],\n ['4.156385 -76.287916']]\n\n\n\n\nJSON\nJSON files are structured as “attribute-value” pairs (aka “key-value” pairs), often with a hierarchical structure. Here’s a brief example:\n    {\n      \"firstName\": \"John\",\n      \"lastName\": \"Smith\",\n      \"isAlive\": true,\n      \"age\": 25,\n      \"address\": {\n        \"streetAddress\": \"21 2nd Street\",\n        \"city\": \"New York\",\n        \"state\": \"NY\",\n        \"postalCode\": \"10021-3100\"\n      },\n      \"phoneNumbers\": [\n        {\n          \"type\": \"home\",\n          \"number\": \"212 555-1234\"\n        },\n        {\n          \"type\": \"office\",\n          \"number\": \"646 555-4567\"\n        }\n      ],\n      \"children\": [],\n      \"spouse\": null\n    }\nA set of key-value pairs is a named array and is placed inside braces (squiggly brackets). Note the nestedness of arrays within arrays (e.g., address within the overarching person array and the use of square brackets for unnamed arrays (i.e., vectors of information), as well as the use of different types: character strings, numbers, null, and (not shown) boolean/logical values. JSON and XML can be used in similar ways, but JSON is less verbose than XML.\nWe can read JSON into Python using the json package. Let’s play again with the Kiva data. The same data that we had worked with in XML format is also available in JSON format: https://api.kivaws.org/v1/loans/newest.json.\n\nURL = \"https://api.kivaws.org/v1/loans/newest.json\"\nresponse = requests.get(URL)\n\nimport json\ndata = json.loads(response.text)\ntype(data)\ndata.keys()\n\ntype(data['loans'])\ndata['loans'][0].keys()\n\ndata['loans'][0]['location']['country']\n[loan['location']['country']  for loan in data['loans']]\n\ndict\n\n\ndict_keys(['paging', 'loans'])\n\n\nlist\n\n\ndict_keys(['id', 'name', 'description', 'status', 'funded_amount', 'basket_amount', 'image', 'activity', 'sector', 'themes', 'use', 'location', 'partner_id', 'posted_date', 'planned_expiration_date', 'loan_amount', 'borrower_count', 'lender_count', 'bonus_credit_eligibility', 'tags'])\n\n\n'Colombia'\n\n\n['Colombia',\n 'Colombia',\n 'Colombia',\n 'Colombia',\n 'Colombia',\n 'Colombia',\n 'Colombia',\n 'Colombia',\n 'Colombia',\n 'Kenya',\n 'Madagascar',\n 'Madagascar',\n 'Madagascar',\n 'Madagascar',\n 'Madagascar',\n 'El Salvador',\n 'Kenya',\n 'Colombia',\n 'Colombia',\n 'Colombia']\n\n\nOne disadvantage of JSON is that it is not set up to deal with missing values, infinity, etc.\n\n\nYAML\nYAML is a similar format commonly used for configuration files that control how code/software/tools behave.\nHere’s an example of the YAML file specifying a GitHub Actions workflow.\nNote the use of indentation (similar to Python) for nesting/hierarchy and the lack of quotation with the strings. This makes it lightweight and readable. Also note the use of arrays/lists and sets of key-value pairs.\n\nimport yaml\n\nwith open(\"book.yml\") as stream:\n   config = yaml.safe_load(stream)  ## `safe_load` avoids running embedded code.\n\nprint(config)\n## How many steps in the `deploy-book` job?\nlen(config['jobs']['deploy-book']['steps'])\n\n{'name': 'deploy-book', True: {'push': {'branches': ['main']}}, 'jobs': {'deploy-book': {'runs-on': 'ubuntu-latest', 'steps': [{'uses': 'actions/checkout@v2'}, {'name': 'Set up Python 3.9', 'uses': 'actions/setup-python@v1', 'with': {'python-version': 3.9}}, {'name': 'Install dependencies', 'run': 'pip install -r book-requirements.txt\\n'}, {'name': 'Build the book', 'run': 'jupyter-book build .\\n'}, {'name': 'GitHub Pages action', 'uses': 'peaceiris/actions-gh-pages@v3', 'with': {'github_token': '${{ secrets.GITHUB_TOKEN }}', 'publish_dir': './_build/html'}}]}}}\n\n\n5\n\n\nNote that (unfortunately) on is treated as a boolean, as discussed in this GitHub issue for the PyYAML package.",
    "crumbs": [
      "Units",
      "Unit 2 (Data technologies)"
    ]
  },
  {
    "objectID": "units/unit2-dataTech.html#webscraping-and-web-apis",
    "href": "units/unit2-dataTech.html#webscraping-and-web-apis",
    "title": "Data technologies, formats, and structures",
    "section": "Webscraping and web APIs",
    "text": "Webscraping and web APIs\nHere we’ll see some examples of making requests over the Web to get data. We’ll use APIs to systematically query a website for information. Ideally, but not always, the API will be documented. In many cases that simply amounts to making an HTTP GET request, which is done by constructing a URL.\nThe requests package is useful for a wide variety of such functionality. Note that much of the functionality I describe below is also possible within the shell using either wget or curl.\n\nWebscraping ethics and best practices\nWebscraping is the process of extracting data from the web, either directly from a website or using a web API (application programming interface).\n\nShould you webscrape? In general, if we can avoid webscraping (particularly if there is not an API) and instead directly download a data file from a website, that is greatly preferred.\nMay you webscrape? Before you set up any automated downloading of materials/data from the web you should make sure that what you are about to do is consistent with the rules provided by the website.\n\nSome places to look for information on what the website allows are:\n\nlegal pages such as Terms of Service or Terms and Conditions on the website.\ncheck the robots.txt file (e.g., https://scholar.google.com/robots.txt) to see what a web crawler is allowed to do, and whether the site requires a particular delay between requests to the sites\npotentially contact the site owner if you plan to scrape a large amount of data\n\nHere are some links with useful information:\n\nBlog post on webscraping ethics\nSome information on how to understand a robots.txt file\n\nTips for when you make automated requests:\n\nWhen debugging code that processes the result of such a request, just run the request once, save (i.e., cache) the result, and then work on the processing code applied to the result. Don’t make the same request over and over again.\nIn many cases you will want to include a time delay between your automated requests to a site, including if you are not actually crawling a site but just want to automate a small number of queries.\n\n\n\nWhat is HTTP?\nHTTP (hypertext transfer protocol) is a system for communicating information from a server (i.e., the website of interest) to a client (e.g., your laptop). The client sends a request and the server sends a response.\nWhen you go to a website in a browser, your browser makes an HTTP GET request to the website. Similarly, when we did some downloading of html from webpages above, we used an HTTP GET request.\nAnytime the URL you enter includes parameter information after a question mark (www.somewebsite.com?param1=arg1&param2=arg2), you are using an API.\nThe response to an HTTP request will include a status code, which can be interpreted based on this information.\nThe response will generally contain content in the form of text (e.g., HTML, XML, JSON) or raw bytes.\n\n\nAPIs: REST- and SOAP-based web services\nIdeally a web service documents their API (Applications Programming Interface) that serves data or allows other interactions. REST and SOAP are popular API standards/styles. Both REST and SOAP use HTTP requests; we’ll focus on REST as it is more common and simpler. When using REST, we access resources, which might be a Facebook account or a database of stock quotes. The API will (hopefully) document what information it expects from the user and will return the result in a standard format (often a particular file format rather than producing a webpage).\nOften the format of the request is a URL (aka an endpoint) plus a query string, passed as a GET request. Let’s search for plumbers near Berkeley, and we’ll see the GET request, in the form:\nhttps://www.yelp.com/search?find_desc=plumbers&find_loc=Berkeley+CA&ns=1\n\nthe query string begins with ?\nthere are one or more Parameter=Argument pairs\npairs are separated by &\n+ is used in place of each space\n\nLet’s see an example of accessing economic data from the World Bank, using the documentation for their API. Following the API call structure, we can download (for example), data on various countries. The documentation indicates that our REST-based query can use either a URL structure or an argument-based structure.\n\n## Queries based on the documentation\napi_url = \"https://api.worldbank.org/V2/incomeLevel/LIC/country\"\napi_args = \"https://api.worldbank.org/V2/country?incomeLevel=LIC\"\n\n## Generalizing a bit\nurl = \"https://api.worldbank.org/V2/country?incomeLevel=MIC&format=json\"\nresponse = requests.get(url)\n\ndata = json.loads(response.content)\n\n## Be careful of data truncation/pagination\nif False:\n    url = \"https://api.worldbank.org/V2/country?incomeLevel=MIC&format=json&per_page=1000\"\n    response = requests.get(url)\n    data = json.loads(response.content)\n\n## Programmatic control\nbaseURL = \"https://api.worldbank.org/V2/country\"\ngroup = 'MIC'\nformat = 'json'\nargs = {'incomeLevel': group, 'format': format, 'per_page': 1000}\nurl = baseURL + '?' + '&'.join(['='.join(\n                               [key, str(args[key])]) for key in args])\nresponse = requests.get(url)\ndata = json.loads(response.content)\n   \ntype(data)\nlen(data[1])\ntype(data[1][5])\ndata[1][5]\n\nlist\n\n\n105\n\n\ndict\n\n\n{'id': 'BEN',\n 'iso2Code': 'BJ',\n 'name': 'Benin',\n 'region': {'id': 'SSF', 'iso2code': 'ZG', 'value': 'Sub-Saharan Africa '},\n 'adminregion': {'id': 'SSA',\n  'iso2code': 'ZF',\n  'value': 'Sub-Saharan Africa (excluding high income)'},\n 'incomeLevel': {'id': 'LMC',\n  'iso2code': 'XN',\n  'value': 'Lower middle income'},\n 'lendingType': {'id': 'IDX', 'iso2code': 'XI', 'value': 'IDA'},\n 'capitalCity': 'Porto-Novo',\n 'longitude': '2.6323',\n 'latitude': '6.4779'}\n\n\nAPIs can change and disappear. A few years ago, the example above involved the World Bank’s Climate Data API, which I can no longer find!\nAs another example, here we can see the US Treasury Department API, which allows us to construct queries for federal financial data.\nThe Nolan and Temple Lang book provides a number of examples of different ways of authenticating with web services that control access to the service.\nFinally, some web services allow us to pass information to the service in addition to just getting data or information. E.g., you can programmatically interact with your Facebook, Dropbox, and Google Drive accounts using REST based on HTTP POST, PUT, and DELETE requests. Authentication is of course important in these contexts and some times you would first authenticate with your login and password and receive a “token”. This token would then be used in subsequent interactions in the same session.\nI created your github.berkeley.edu accounts from Python by interacting with the GitHub API using requests.\n\n\nHTTP requests by deconstructing an (undocumented) API\nIn some cases an API may not be documented or we might be lazy and not use the documentation. Instead we might deconstruct the queries a browser makes and then mimic that behavior, in some cases having to parse HTML output to get at data. Note that if the webpage changes even a little bit, our carefully constructed query syntax may fail.\nLet’s look at some UN data (agricultural crop data). By going to\nhttps://data.un.org/Explorer.aspx?d=FAO, and clicking on “Crops”, we’ll see a bunch of agricultural products with “View data” links. Click on “apricots” as an example and you’ll see a “Download” button that allows you to download a CSV of the data. Let’s select a range of years and then try to download “by hand”. Sometimes we can right-click on the link that will download the data and directly see the URL that is being accessed and then one can deconstruct it so that you can create URLs programmatically to download the data you want.\nIn this case, we can’t see the full URL that is being used because there’s some Javascript involved. Therefore, rather than looking at the URL associated with a link we need to view the actual HTTP request sent by our browser to the server. We can do this using features of the browser (e.g., in Firefox see Web Developer -&gt; Network and in Chrome View -&gt; Developer -&gt; Developer tools and choose the Network tab) (or right-click on the webpage and select Inspect and then Network). Based on this we can see that an HTTP GET request is being used with a URL such as:\nhttp://data.un.org/Handlers/DownloadHandler.ashx?DataFilter=itemCode:526;year:2012,2013,2014,2015,2016,2017&DataMartId=FAO&Format=csv&c=2,4,5,6,7&s=countryName:asc,elementCode:asc,year:desc.\nWe’e now able to easily download the data using that URL, which we can fairly easily construct using string processing in bash, Python, or R, such as this (here I just paste it together directly, but using more structured syntax such as I used for the World Bank example would be better):\n\nimport zipfile\n\n## example URL:\n## https://data.un.org/Handlers/DownloadHandler.ashx?DataFilter=itemCode:526;\n##year:2012,2013,2014,2015,2016,2017&DataMartId=FAO&Format=csv&c=2,4,5,6,7&\n##s=countryName:asc,elementCode:asc,year:desc\nitemCode = 526\nbaseURL = \"https://data.un.org/Handlers/DownloadHandler.ashx\"\nyrs = ','.join([str(yr) for yr in range(2012,2018)])\nfilter = f\"?DataFilter=itemCode:{itemCode};year:{yrs}\"\nargs1 = \"&DataMartId=FAO&Format=csv&c=2,3,4,5,6,7&\"\nargs2 = \"s=countryName:asc,elementCode:asc,year:desc\"\nurl = baseURL + filter + args1 + args2\n## If the website provided a CSV, this would be easier, but it zips the file.\nresponse = requests.get(url)\n\nwith io.BytesIO(response.content) as stream:  # create a file-like object\n     with zipfile.ZipFile(stream, 'r') as archive:   # treat the object as a zip file\n          with archive.open(archive.filelist[0].filename, 'r') as file:  # get a pointer to the embedded file\n              dat = pd.read_csv(file)\n\ndat.head()\n\n\n\n\n\n\n\n\nCountry or Area\nElement Code\nElement\nYear\nUnit\nValue\nValue Footnotes\n\n\n\n\n0\nAfghanistan\n432\nGross Production Index Number (2014-2016 = 100)\n2017.0\nIndex\n202.19\nNaN\n\n\n1\nAfghanistan\n432\nGross Production Index Number (2014-2016 = 100)\n2016.0\nIndex\n27.45\nNaN\n\n\n2\nAfghanistan\n432\nGross Production Index Number (2014-2016 = 100)\n2015.0\nIndex\n134.50\nNaN\n\n\n3\nAfghanistan\n432\nGross Production Index Number (2014-2016 = 100)\n2014.0\nIndex\n138.05\nNaN\n\n\n4\nAfghanistan\n432\nGross Production Index Number (2014-2016 = 100)\n2013.0\nIndex\n138.05\nNaN\n\n\n\n\n\n\n\nSo, what have we achieved?\n\nWe have a reproducible workflow we can share with others (perhaps ourself in the future).\nWe can automate the process of downloading many such files.\n\n\n\nMore details on HTTP requests\nA more sophisticated way to do the download is to pass the request in a structured way with named input parameters. This request is easier to construct programmatically. Here what is returned is a zip file, which is represented in Python as a sequence of “raw” bytes.\n\ndata = {\"DataFilter\": f\"itemCode:{itemCode};year:{yrs}\",\n       \"DataMartID\": \"FAO\", \n       \"Format\": \"csv\", \n       \"c\": \"2,3,4,5,6,7\",\n       \"s\": \"countryName:asc,elementCode:asc,year:desc\"\n       }    \n\nresponse = requests.get(baseURL, params = data)\n\nwith io.BytesIO(response.content) as stream:  \n     with zipfile.ZipFile(stream, 'r') as archive:\n          with archive.open(archive.filelist[0].filename, 'r') as file:  \n              dat = pd.read_csv(file)\n\nIn some cases we may need to send a lot of information as part of the URL in a GET request. If it gets to be too long (e.g,, more than 2048 characters) many web servers will reject the request. Instead we may need to use an HTTP POST request (POST requests are often used for submitting web forms). A typical request would have syntax like this search (using requests):\n\nurl = 'http://www.wormbase.org/db/searches/advanced/dumper'\n\ndata = {      \"specipes\":\"briggsae\",\n              \"list\": \"\",\n              \"flank3\": \"0\",\n              \"flank5\": \"0\",\n              \"feature\": \"Gene Models\",\n              \"dump\": \"Plain TEXT\",\n              \"orientation\": \"Relative to feature\",\n              \"relative\": \"Chromsome\",\n              \"DNA\":\"flanking sequences only\",\n              \".cgifields\" :  \"feature, orientation, DNA, dump, relative\"\n}                                  \n\nresponse = requests.post(url, data = data)\nif response.status_code == 200:\n    print(\"POST request successful\")\nelse:\n    print(f\"POST request failed with status code: {response.status_code}\")\n\nUnfortunately that specific search doesn’t work because the server URL and/or API seem to have changed. But it gives you an idea of what the format would look like.\nrequests can handle other kinds of HTTP requests such as PUT and DELETE. Finally, some websites use cookies to keep track of users, and you may need to download a cookie in the first interaction with the HTTP server and then send that cookie with later interactions. More details are available in the Nolan and Temple Lang book.\n\n\nPackaged access to an API\nFor popular websites/data sources, a developer may have packaged up the API calls in a user-friendly fashion as functions for use from Python, R, or other software. For example there are Python (twitter) and R (twitteR) packages for interfacing with Twitter via its API. (Actually, with the change to X, I don’t know if this API is still available.)\nHere’s some example code for Python. This looks up the US senators’ Twitter names and then downloads a portion of each of their timelines, i.e., the time series of their tweets. Note that Twitter has limits on how much one can download at once.\n\nimport json\nimport twitter\n\n# You will need to set the following variables with your\n# personal information.  To do this you will need to create\n# a personal account on Twitter (if you don't already have\n# one).  Once you've created an account, create a new\n# application here:\n#    https://dev.twitter.com/apps\n#\n# You can manage your applications here:\n#    https://apps.twitter.com/\n#\n# Select your application and then under the section labeled\n# \"Key and Access Tokens\", you will find the information needed\n# below.  Keep this information private.\nCONSUMER_KEY       = \"\"\nCONSUMER_SECRET    = \"\"\nOAUTH_TOKEN        = \"\"\nOAUTH_TOKEN_SECRET = \"\"\n\nauth = twitter.oauth.OAuth(OAUTH_TOKEN, OAUTH_TOKEN_SECRET,\n                           CONSUMER_KEY, CONSUMER_SECRET)\napi = twitter.Twitter(auth=auth)\n\n# get the list of senators\nsenators = api.lists.members(owner_screen_name=\"gov\",\n                             slug=\"us-senate\", count=100)\n\n# get all the senators' timelines\nnames = [d[\"screen_name\"] for d in senators[\"users\"]]\ntimelines = [api.statuses.user_timeline(screen_name=name, count = 500) \n             for name in names]\n\n# save information out to JSON\nwith open(\"senators-list.json\", \"w\") as f:\n    json.dump(senators, f, indent=4, sort_keys=True)\nwith open(\"timelines.json\", \"w\") as f:\n    json.dump(timelines, f, indent=4, sort_keys=True)\n\n\n\nAccessing dynamic pages\nMany websites dynamically change in reaction to the user behavior. In these cases you need a tool that can mimic the behavior of a human interacting with a site. Some options are:\n\nselenium is a popular tool for doing this, and there is a Python package of the same name.\nUsing scrapy plus splash is another approach.",
    "crumbs": [
      "Units",
      "Unit 2 (Data technologies)"
    ]
  },
  {
    "objectID": "units/unit2-dataTech.html#standard-data-structures-in-python-and-r",
    "href": "units/unit2-dataTech.html#standard-data-structures-in-python-and-r",
    "title": "Data technologies, formats, and structures",
    "section": "Standard data structures in Python and R",
    "text": "Standard data structures in Python and R\n\nIn Python and R, one often ends up working with dataframes, lists, and arrays/vectors/matrices/tensors.\nIn Python we commonly work with data structures that are part of additional packages, in particular numpy arrays and pandas dataframes.\nDictionaries in Python allow for easy use of key-value pairs where one can access values based on their key/label. In R one can do something similar with named vectors or named lists or (more efficiently) by using environments.\nIn R, if we are not working with rectangular datasets or standard numerical objects, we often end up using lists or enhanced versions of lists, sometimes with deeply nested structures.\n\nIn Unit 7, we’ll talk about distributed data structures that allow one to easily work with data distributed across multiple computers.",
    "crumbs": [
      "Units",
      "Unit 2 (Data technologies)"
    ]
  },
  {
    "objectID": "units/unit2-dataTech.html#other-kinds-of-data-structures",
    "href": "units/unit2-dataTech.html#other-kinds-of-data-structures",
    "title": "Data technologies, formats, and structures",
    "section": "Other kinds of data structures",
    "text": "Other kinds of data structures\nYou may have heard of various other kinds of data structures, such as linked lists, trees, graphs, queues, and stacks. One of the key aspects that differentiate such data structures is how one navigates through the elements.\nSets are collections of elements that don’t have any duplicates (like a mathematical set).\nWith a linked list, with each element (or node) has a value and a pointer (reference) to the location of the next element. (With a doubly-linked list, there is also a pointer back to the previous element.) One big advantage of this is that one can insert an element by simply modifying the pointers involved at the site of the insertion, without copying any of the other elements in the list. A big disadvantage is that to get to an element you have to navigate through the list.\n\n\n\nLinked list (courtesy of computersciencewiki.org)\n\n\nBoth trees and graphs are collections of nodes (vertices) and links (edges). A tree involves a set of nodes and links to child nodes (also possibly containing information linking the child nodes to their parent nodes). With a graph, the links might not be directional, and there can be cycles.\n\n\n\nTree (courtesy of computersciencewiki.org)\n\n\n\n\n\nGraph (courtesy of computersciencewiki.org)\n\n\nA stack is a collection of elements that behave like a stack of lunch trays. You can only access the top element directly(“last in, first out”), so the operations are that you can push a new element onto the stack or pop the top element off the stack. In fact, nested function calls behave as stacks, and the memory used in the process of evaluating the function calls is called the ‘stack’.\nA queue is like the line at a grocery store, behaving as “first in, first out”.\nOne can use such data structures either directly or via add-on packages in Python and R, though I don’t think they’re all that commonly used in R. This is probably because statistical/data science/machine learning workflows often involve either ‘rectangular’ data (i.e., dataframe-style data) and/or mathematical computations with arrays. That said, trees and graphs are widely used.\nSome related concepts that we’ll discuss further in Unit 5 include:\n\ntypes: this refers to how a given piece of information is stored and what operations can be done with the information.\n\n‘primitive’ types are the most basic types that often relate directly to how data are stored in memory or on disk (e.g., booleans, integers, numeric (real-valued), character, pointer (address, reference).\n\npointers: references to other locations (addresses) in memory. One often uses pointers to avoid unnecessary copying of data.\nhashes: hashing involves fast lookup of the value associated with a key (a label), using a hash function, which allows one to convert the key to an address. This avoids having to find the value associated with a specific key by looking through all the keys until the key of interest is found (an O(n) operation).",
    "crumbs": [
      "Units",
      "Unit 2 (Data technologies)"
    ]
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Statistics 243 is an introduction to statistical computing, taught using Python. The course will cover both programming concepts and statistical computing concepts. Programming concepts will include data and text manipulation, regular expressions, data structures, functions and variable scope, memory use, efficiency, debugging, testing, and parallel processing. Statistical computing topics will include working with large datasets, numerical linear algebra, computer arithmetic/precision, simulation studies and Monte Carlo methods, and numerical optimization. A goal is that coverage of these topics complement the models/methods discussed in the rest of the statistics/biostatistics graduate curriculum. We will also cover the basics of UNIX/Linux, in particular shell scripting and operating on remote servers, as well as a bit of R.\n\n\n\nWhile the course is taught using Python and you will learn a lot about using Python at an advanced level, this is not a course about learning Python. Rather the focus of the course is computing for statistics and data science more generally, using Python to illustrate the concepts.\nThis is not a course that will cover specific statistical/machine learning/data analysis methods.\n\n\n\n\nInformal prerequisites: If you are not a statistics or biostatistics graduate student, please chat with me if you’re not sure if this course makes sense for you. A background in calculus, linear algebra, probability and statistics is expected, as well as a basic ability to operate on a computer (but I do not assume familiarity with the UNIX-style command line/terminal/shell). Furthermore, I’m expecting you will know the basics of Python, at the level of the Python material in our computing skills workshop offered Aug. 20-21, 2024. If you don’t have that background you’ll need to spend time in the initial couple weeks getting up to speed. The workshop materials are a good resource.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-description",
    "href": "syllabus.html#course-description",
    "title": "Syllabus",
    "section": "",
    "text": "Statistics 243 is an introduction to statistical computing, taught using Python. The course will cover both programming concepts and statistical computing concepts. Programming concepts will include data and text manipulation, regular expressions, data structures, functions and variable scope, memory use, efficiency, debugging, testing, and parallel processing. Statistical computing topics will include working with large datasets, numerical linear algebra, computer arithmetic/precision, simulation studies and Monte Carlo methods, and numerical optimization. A goal is that coverage of these topics complement the models/methods discussed in the rest of the statistics/biostatistics graduate curriculum. We will also cover the basics of UNIX/Linux, in particular shell scripting and operating on remote servers, as well as a bit of R.\n\n\n\nWhile the course is taught using Python and you will learn a lot about using Python at an advanced level, this is not a course about learning Python. Rather the focus of the course is computing for statistics and data science more generally, using Python to illustrate the concepts.\nThis is not a course that will cover specific statistical/machine learning/data analysis methods.\n\n\n\n\nInformal prerequisites: If you are not a statistics or biostatistics graduate student, please chat with me if you’re not sure if this course makes sense for you. A background in calculus, linear algebra, probability and statistics is expected, as well as a basic ability to operate on a computer (but I do not assume familiarity with the UNIX-style command line/terminal/shell). Furthermore, I’m expecting you will know the basics of Python, at the level of the Python material in our computing skills workshop offered Aug. 20-21, 2024. If you don’t have that background you’ll need to spend time in the initial couple weeks getting up to speed. The workshop materials are a good resource.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#objectives-of-the-course",
    "href": "syllabus.html#objectives-of-the-course",
    "title": "Syllabus",
    "section": "Objectives of the course",
    "text": "Objectives of the course\nThe goals of the course are that, by the end of the course, students be able to:\n\noperate effectively in a UNIX environment and on remote servers and compute clusters;\nhave a solid understanding of general programming concepts and principles, and be able to program effectively (including having an advanced knowledge of Python functionality);\nbe familiar with concepts and tools for reproducible research and good scientific computing practices; and\nunderstand in depth and be able to make use of principles of numerical linear algebra, optimization, and simulation for statistics- and data science-related analyses and research.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#topics-in-order-with-rough-timing",
    "href": "syllabus.html#topics-in-order-with-rough-timing",
    "title": "Syllabus",
    "section": "Topics (in order with rough timing)",
    "text": "Topics (in order with rough timing)\nThe ‘days’ here are (roughly) class sessions, as general guidance.\n\nIntroduction to UNIX, operating on a compute server (1 day)\nData formats, data access, webscraping, data structures (2 days)\nDebugging, good programming practices, reproducible research (1 day)\nThe bash shell and shell scripting, version control (3 days)\nProgramming concepts and advanced Python programming: text processing and regular expressions, object-oriented programming, functions and variable scope, memory use, efficient programming (9 days)\nParallel processing (2 days)\nWorking with databases, hashing, and big data (3 days)\nComputer arithmetic/representation of numbers on a computer (3 days)\nSimulation studies and Monte Carlo (2 days)\nNumerical linear algebra (5 days)\nOptimization (5 days)\nGraphics (1 day)",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#personnel",
    "href": "syllabus.html#personnel",
    "title": "Syllabus",
    "section": "Personnel",
    "text": "Personnel\n\nInstructor:\n\nChris Paciorek (paciorek@stat.berkeley.edu)\n\nGSI\n\nJoão Vitor Romano (jv.romano@berkeley.edu)\n\nOffice hours can be found here.\nWhen to see us about an assignment: We’re here to help, including providing guidance on assignments. You don’t want to be futilely spinning your wheels for a long time getting nowhere. That said, before coming to see us about a difficulty, you should try something a few different ways and define/summarize for yourself what is going wrong or where you are getting stuck.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-websites-github-piazza-gradescope-and-bcourses",
    "href": "syllabus.html#course-websites-github-piazza-gradescope-and-bcourses",
    "title": "Syllabus",
    "section": "Course websites: GitHub, Ed Discussion, Gradescope, and bCourses",
    "text": "Course websites: GitHub, Ed Discussion, Gradescope, and bCourses\nKey websites for the course are:\n\nThis course website, which is hosted on GitHub pages, and the GitHub repository containing the source materials: https://github.com/berkeley-stat243/fall-2024\nSCF tutorials for additional content: https://statistics.berkeley.edu/computing/training/tutorials\nEd Discussion site for discussions/Q&A: https://edstem.org/us/courses/62469/discussion\nbCourses site for course capture recordings (see Media Gallery) and possibly some other materials: https://bcourses.berkeley.edu/courses/1537551\nGradescope for assignments (also linked from bCourses): https://www.gradescope.com/courses/825461\n\nAll course materials will be posted on here on the website (and on GitHub) except for video content, which will be in bCourses.\n\nCourse discussion\nWe will use the course Ed Discussion site for communication (announcements, questions, and discussion). You should ask questions about class material and problem sets through the site. Please use this site for your questions so that either or I can respond and so that everyone can benefit from the discussion. I suggest you to modify your settings on Ed Discussion so you are informed by email of postings. In particular you are responsible for keeping track of all course announcements, which we’ll make on the Discussion forum. I strongly encourage you to respond to or comment on each other’s questions as well (this will help your class participation grade), although of course you should not provide a solution to a problem set problem. If you have a specific administrative question you need to direct just to me, it’s fine to email me directly or post privately on the Discussion site. But if you simply want to privately ask a question about content, then just come to an office hour or see me after class or João during/after section.\nIf you’re enrolled in the class you should be a member of the group and be able to access it. If you’re auditing or not yet enrolled and would like access, make sure to fill out the course survey and I will add you. In addition, we will use Gradescope for viewing grades.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-material",
    "href": "syllabus.html#course-material",
    "title": "Syllabus",
    "section": "Course material",
    "text": "Course material\n\nPrimary materials: Course notes on course webpage/GitHub, SCF tutorials, and potentially pre-recorded videos on bCourses.\nBack-up textbooks (generally available via UC Library via links below):\n\nFor bash: Newham, Cameron and Rosenblatt, Bill. Learning the bash Shell available electronically through UC Library\nFor Quarto: The Quarto reference guide\nFor statistical computing topics:\n\nGentle, James. Computational Statistics\nGentle, James. Matrix Algebra or Numerical Linear Algebra with Applications in Statistics\n\nOther resources with more detail on particular aspects of statistical computing concepts:\n\nLange, Kenneth; Numerical Analysis for Statisticians, 2nd ed. First edition available through UC library\nMonahan, John; Numerical Methods of Statistics",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#section",
    "href": "syllabus.html#section",
    "title": "Syllabus",
    "section": "Section",
    "text": "Section\nThe GSI will lead a two-hour discussion section each week (there are two sections). By and large, these will only last for about one hour of actual content, but the second hour may be used as an office hour with the GSI or for troubleshooting software during the early weeks. The discussion sections will vary in format and topic, but material will include demonstrations on various topics (version control, debugging, testing, etc.), group work on these topics, discussion of relevant papers, and discussion of problem set solutions. The first section (12-2 pm) generally has more demand, so to avoid having too many people in the room, you should go to your assigned section unless you talk to me first.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#computing-resources",
    "href": "syllabus.html#computing-resources",
    "title": "Syllabus",
    "section": "Computing Resources",
    "text": "Computing Resources\nMost work for the course can be done on your laptop. Later in the course we’ll also use the Statistics Department Linux cluster. You can also use the SCF JupyterHub or the campus DataHub to access a bash shell or run an IPython notebook. (The campus DataHub is limited in terms of number of CPU cores and memory so won’t be suitable for more computationally-intensive work later in the semester.)\nThe software needed for the course is as follows:\n\nAccess to the UNIX command line (bash shell)\nGit\nPython (the Miniforge installation of Conda is recommended but by no means required)\nQuarto\n\nSee the “how tos” in the left sidebar for tips on software installation and access to a UNIX shell, which you’ll need to be able to do by the second week of class.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#class-time",
    "href": "syllabus.html#class-time",
    "title": "Syllabus",
    "section": "Class time",
    "text": "Class time\nMy goal is to have classes be an interactive environment. This is both more interesting for all of us and more effective in learning the material. I encourage you to ask questions and will pose questions to the class to think about, respond to via online polling or Google forms, and discuss. To increase time for discussion and assimilation of the material in class, before some classes I may ask that you read material or work through tutorials in advance of class. Occasionally, I will ask you to submit answers to questions in advance of class as well.\nPlease do not use phones during class and limit laptop use to the material being covered.\nStudent backgrounds with computing will vary. For those of you with limited background on a topic, I encourage you to ask questions during class so I know what you find confusing. For those of you with extensive background on a topic (there will invariably be some topics where one of you will know more about it than I do or have more real-world experience), I encourage you to pitch in with your perspective. In general, there are many ways to do things on a computer, particularly in a UNIX environment and in Python, so it will help everyone (including me) if we hear multiple perspectives/ideas.\nFinally, class recordings for review or to make up for absence will be available through the bCourses Media Gallery, available on the Media Gallery tab on the bCourses page for the class.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-requirements-and-grading",
    "href": "syllabus.html#course-requirements-and-grading",
    "title": "Syllabus",
    "section": "Course requirements and grading",
    "text": "Course requirements and grading\n\nScheduling Conflicts\nCampus asks that I include this information about conflicts: Please notify me in writing by the second week of the term about any known or potential extracurricular conflicts (such as religious observances, graduate or medical school interviews, or team activities). I will try my best to help you with making accommodations, but I cannot promise them in all cases. In the event there is no mutually-workable solution, you may be dropped from the class.\nThe main conflict that would be a problem would be the quizzes, whose dates I will determine in late August / early September.\nQuizzes are in-person. There is no remote option, and the only make-up accommodations I will make are for illness or serious personal issues. Do not schedule any travel that may conflict with a quiz.\n\n\nCourse grades\nThe grade for this course is primarily based on assignments due every 1-2 weeks, two quizzes (likely in early-mid October and mid-late November), and a final group project. I will also provide extra credit questions on some problem sets. There is no final exam.\n\n50% of the grade is based on the problem sets,\n25% on the quizzes,\n15% on the project, and\n10% on your participation in discussions on Ed (i.e., responding to your classmates’ questions and asking thoughtful questions about course material), your responses to the in-class Google forms questions, completion of occasional non-problem set assignments (including assignments in lab section), as well as occasional brief questions that I will ask you to answer in advance of the next class.\n\nGrades will generally be As and Bs. An A involves doing all the work, getting full credit on most of the problem sets, doing well on the quizzes, and doing a thorough job on the final project.\n\n\nProblem sets\nWe will be less willing to help you if you come to our office hours or post a question online at the last minute. Working with computers can be unpredictable, so give yourself plenty of time for the assignments.\nThere are several rules for submitting your assignments.\n\nYou should prepare your assignments using Quarto.\nProblem set submission consists of both of the following:\n\nA PDF submitted electronically through Gradescope, by the start of class (10 am) on the due date, and\nAn electronic copy of the PDF, code files, and Quarto document pushed to your class GitHub repository, following the instructions to be provided by the GSI.\n\nOn-time submission will be determined based on the time stamp of when the PDF is submitted to Gradescope.\nAnswers should consist of textual response or mathematical expressions as appropriate, with key chunks of code embedded within the document. Function definitions would generally be placed in a separate .py code file. The function definitions and any extensive additional code should be provided as an appendix. Before diving into the code for a problem, you should say what the goal of the code is and your strategy for solving the problem. Raw code without explanation is not an appropriate solution. Please see our qualitative grading rubric for guidance. In general the rubric is meant to reinforce good coding practices and high-quality scientific communication.\nAny mathematical derivations may be done by hand and scanned with your phone if you prefer that to writing up LaTeX equations.\nYou must include a “Collaboration” section in which you indicate any other students you worked with. If you did not collaborate with anyone else, simply state that.\n\nNote: Quarto Markdown is an extension to the Markdown markup language that allows one to embed Python and R code within an HTML document. Please see the SCF dynamics document tutorial; there will be additional information in the first section and on the first problem set.\n\n\nSubmitting assignments\nIn the first section (September 1), we’ll discuss how to submit your problem sets both on Gradescope and via your class GitHub repository, located at https://github.berkeley.edu/&lt;your_calnet_username&gt;.\n\n\nProblem set grading\nThe grading scheme for problem sets is as follows. Each problem set will receive a numeric score for (1) presentation and explanation of results, (2) technical accuracy of code or mathematical derivation, and (3) code quality (style, structure, reproducibility, and creativity). For each of these three components, the possible scores are:\n\n0 = no credit,\n1 = partial credit (you did some of the problems but not all),\n2 = satisfactory (you tried everything but there were pieces of what you did that didn’t completely/correctly solve or present/explain one or more problems), and\n3 = full credit.\n\nAgain, the qualitative grading rubric provides guidance on what we want to see for full credit.\nFor components #1 and #3, many of you will get a score of 2 for some problem sets as you develop good presentation and coding practices. You can still get an A in the class despite this.\nYour total score for the PS is a weighted sum of the scores for the three components. If you turn in a PS late, I’ll bump you down by two points (out of the available). If you turn it in really late (e.g., after we start grading them), I will bump you down by four points. No credit after solutions are distributed.\n\n\nFinal project\nThe final project will be a joint coding project in groups of 3-4. I’ll assign an overall task, and you’ll be responsible for dividing up the work, coding, debugging, testing, and documentation. You’ll need to use the Git version control system for working in your group.\n\n\nRules for working together and the campus honor code\nI encourage you to work together and help each other out. However, the problem set solutions you submit must be your own. What do I mean by that?\n\nYou should first try to figure out a given problem on your own. After that, if you’re stuck or want to explore alternative approaches or check what you’ve done, feel free to consult with your fellow students and with the GSI and me.\nWhat does “consult with a fellow student mean”? You can discuss a problem with another student, brainstorm approaches, and share code syntax (generally not more than one line) on how to do small individual coding tasks within a problem.\n\nYou should not ask another student for complete code or solutions, or look at their code/solution.\nYou should not share complete code or solutions with another student or on Ed Discussion.\n\nYou may use ChatGPT (or similar chatbots) for help with small sections of a problem (e.g., how to do some specific Python or bash task). You should not use ChatGPT to try to answer an entire question. You should carefully verify that the result is correct.\nYou must provide attribution for ideas obtained elsewhere, including other students and ChatGPT or similar chatbots.\n\nIf you got a specific idea for how to do part of a problem from a fellow student (or some other resource, including ChatGPT), you should note that in your solution in the appropriate place (for specific syntax ideas, note this in a code comment), just as you would cite a book or URL.\nIn addition, you MUST list in a Collaboration section on your problem set solution any fellow students who you worked/consulted with.\nYou do not need to cite any Ed Discussion posts nor any discussions with Chris or João.\n\nUltimately, your solution to a problem set (writeup and code) must be your own, and you’ll hear from me if either look too similar to someone else’s.\n\nPlease see the last section of this document for more information on the Campus Honor Code, which I expect you to follow.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#feedback",
    "href": "syllabus.html#feedback",
    "title": "Syllabus",
    "section": "Feedback",
    "text": "Feedback\nI welcome comments and suggestions and concerns. Particularly good suggestions will count towards your class participation grade.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#accomodations-for-students-with-disabilities",
    "href": "syllabus.html#accomodations-for-students-with-disabilities",
    "title": "Syllabus",
    "section": "Accomodations for Students with Disabilities",
    "text": "Accomodations for Students with Disabilities\nPlease see me as soon as possible if you need particular accommodations, and we will work out the necessary arrangements.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#campus-honor-code",
    "href": "syllabus.html#campus-honor-code",
    "title": "Syllabus",
    "section": "Campus Honor Code",
    "text": "Campus Honor Code\nThe following is the Campus Honor Code. With regard to collaboration and independence, please see my rules regarding problem sets above – Chris.\nThe student community at UC Berkeley has adopted the following Honor Code: “As a member of the UC Berkeley community, I act with honesty, integrity, and respect for others.” The hope and expectation is that you will adhere to this code.\nCollaboration and Independence: Reviewing lecture and reading materials and studying for exams can be enjoyable and enriching things to do with fellow students. This is recommended. However, unless otherwise instructed, homework assignments are to be completed independently and materials submitted as homework should be the result of one’s own independent work.\nCheating: A good lifetime strategy is always to act in such a way that no one would ever imagine that you would even consider cheating. Anyone caught cheating on a quiz or exam in this course will receive a failing grade in the course and will also be reported to the University Center for Student Conduct. In order to guarantee that you are not suspected of cheating, please keep your eyes on your own materials and do not converse with others during the quizzes and exams.\nPlagiarism: To copy text or ideas from another source without appropriate reference is plagiarism and will result in a failing grade for your assignment and usually further disciplinary action. For additional information on plagiarism and how to avoid it, see, for example: http://gsi.berkeley.edu/teachingguide/misconduct/prevent-plag.html\nAcademic Integrity and Ethics: Cheating on exams and plagiarism are two common examples of dishonest, unethical behavior. Honesty and integrity are of great importance in all facets of life. They help to build a sense of self-confidence, and are key to building trust within relationships, whether personal or professional. There is no tolerance for dishonesty in the academic world, for it undermines what we are dedicated to doing – furthering knowledge for the benefit of humanity.\nYour experience as a student at UC Berkeley is hopefully fueled by passion for learning and replete with fulfilling activities. And we also appreciate that being a student may be stressful. There may be times when there is temptation to engage in some kind of cheating in order to improve a grade or otherwise advance your career. This could be as blatant as having someone else sit for you in an exam, or submitting a written assignment that has been copied from another source. And it could be as subtle as glancing at a fellow student’s exam when you are unsure of an answer to a question and are looking for some confirmation. One might do any of these things and potentially not get caught. However, if you cheat, no matter how much you may have learned in this class, you have failed to learn perhaps the most important lesson of all.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "labs/lab0-setup.html",
    "href": "labs/lab0-setup.html",
    "title": "Lab 0: Setup",
    "section": "",
    "text": "Warning: This page is under construction for 2024.",
    "crumbs": [
      "Labs",
      "Lab 0 (Optional setup)"
    ]
  },
  {
    "objectID": "labs/lab0-setup.html#setting-up-your-environment-082523",
    "href": "labs/lab0-setup.html#setting-up-your-environment-082523",
    "title": "Lab 0: Setup",
    "section": "Setting up your environment (08/25/23)",
    "text": "Setting up your environment (08/25/23)\nYou will need to set up (or make sure you have access to) the following:\n\nUnix shell\nGit\nQuarto\nPython\nA text editor or IDE of your choice (deeb recommends VS Code or sublime, but if you are already familiar with a specific editor, stick with it)\n\nAfter making sure you have access to all these 5 tools, it may be a good idea to go through some of the following tutorials on unix bash and unix commands. You can do this on your own or in the Lab section on Friday 8/25.\nAttending lab 0 is optional. If you successfully set up your environment to have all the listed tools, you don’t need to attend. You are welcome to attend to ask for help with the setup or to help your classmates with setting up their environments.\nYou are also welcome to come and ask for help on using any of these 5 tools/systems (esp. bash and unix commands this week), but priority will be given to environment setup questions.\nReach out to deeb @ deeb@berkeley.edu with any unresolved problems or if you discover something that needs to be changed with our howtos and instructions.\nDeeb’s unsolicited advice on languages and tools:\n\nWhichever editor you pick, make sure to spend some time every week learning a few keyboard shortcuts for it. The same goes for the bash shell (ctrl+a and ctrl+e are among my favorites) and for your OS of choice in general (and even your web browser!). Not only do keyboard shortcuts make you more efficient, but they dramatically reduce the cognitive load after a while, and so make your life less painful in the long run. They can be the difference between hating computers and loving them.\nProgramming languages come and go, but Unix is forever! Well, maybe not forever, but close enough. Invest more of your time in getting familiar with durable and proven paradigms. Different programming languages are suitable in different situations and change dramatically from one decade to the next, but the unix shell and commands are as pristine, long lived, and as widely applicable as you’ll find in the computing world. I have a much more mixed view of git, Python, R, and C++. Another example of a very durable computing paradigm is SQL, which we’ll get to in a few weeks.",
    "crumbs": [
      "Labs",
      "Lab 0 (Optional setup)"
    ]
  },
  {
    "objectID": "calendar.html",
    "href": "calendar.html",
    "title": "Calendar",
    "section": "",
    "text": "We can just embed the iframe html:"
  },
  {
    "objectID": "howtos/RandRStudioInstall.html",
    "href": "howtos/RandRStudioInstall.html",
    "title": "Installing R & RStudio",
    "section": "",
    "text": "On your laptop\nIf your version of R is older than 4.0.0, please install the latest version.\nTo install R, see:\n\nMacOS: install the R-4.2.1.pkg from https://cran.rstudio.com/bin/macosx\nWindows: https://cran.rstudio.com/bin/windows/base/\nLinux: https://cran.rstudio.com/bin/linux/\n\nThen install RStudio. To do so, see https://www.rstudio.com/ide/download/desktop, scrolling down to the “Installers for Supported Platforms” section and selecting the Installer for your operating system.\nVerify that you can install add-on R packages by installing the ‘fields’ package. In RStudio, go to ‘Tools-&gt;Install Packages’. In the resulting dialog box, enter ‘fields’ (without quotes) in the ‘Packages’ field. Depending on the location specified in the ‘Install to Library’ field, you may need to enter your administrator password. To be able to install packages to the directory of an individual user, you may need to do the following:\n\nIn R, enter the command Sys.getenv()['R_LIBS_USER'].\nCreate the directory specified in the result that R returns, e.g., on a Mac, this might be ~/Library/R/4.0/library.\n\nFor more detailed installation instructions for Windows, see Using R, RStudio, and LaTeX on Windows file.\n\n\nVia DataHub\nSee the instructions in Accessing the Unix Command Line for how to login to Datahub. Then in the mid-upper right, click on New and RStudio. Alternatively, to go directly to RStudio, go to https://r.datahub.berkeley.edu."
  },
  {
    "objectID": "howtos/submitPS.html",
    "href": "howtos/submitPS.html",
    "title": "Problem Set Submissions",
    "section": "",
    "text": "These requirements are still being finalized. Do not rely on this material yet.",
    "crumbs": [
      "How tos",
      "Problem Set Submissions"
    ]
  },
  {
    "objectID": "howtos/submitPS.html#submission-format",
    "href": "howtos/submitPS.html#submission-format",
    "title": "Problem Set Submissions",
    "section": "Submission format",
    "text": "Submission format\nProblem set solutions should be written in Quarto Markdown (.qmd) source files, interspersing explanatory text with Python (and in some cases bash) code chunks. Please do not use Jupyter notebook (.ipynb) files as your underlying source file for your solutions. In some cases we will ask that you put function definitions for more complicated functions into one or more Python code (.py) file(s) and show us the code in the appendix of your main solution file by using inspect.getsource().\nWhy?\n\nFor one or two of the initial problem sets you’ll need to include both bash and Python code. This isn’t possible in a single notebook.\nThe underlying format of .ipynb files is JSON. While this is a plain text format, the key-value pair structure (not generally being aligned with the file lines) is much less well-suited for use with Git version control (which relies on diff) than Markdown-based formats.\nOne can run chunks in a Jupyter notebook in arbitrary order. What is printed to PDF depends on the order in which the chunks are run and the results can differ from what one would expect based on reading the notebook sequentially and running the chunks sequentially. For example, consider the following experiment and you’ll see what I mean: (1) Have one code chunk with a = 3 and run it; (2) Add a second chunk with print(a) and run it; and (3) Change the first chunk to a=4 and DO NOT rerun the second chunk. Save the notebook to PDF. You’ll see that your “report” makes no sense. Here’s the result of me doing that experiment.\n\nIf you really want to do your initial explorations of the problems in a Jupyter notebook, with content then copied to qmd, that is fine.",
    "crumbs": [
      "How tos",
      "Problem Set Submissions"
    ]
  },
  {
    "objectID": "howtos/submitPS.html#problem-set-solution-workflows",
    "href": "howtos/submitPS.html#problem-set-solution-workflows",
    "title": "Problem Set Submissions",
    "section": "Problem set solution workflows",
    "text": "Problem set solution workflows\nHere we outline a few suggested workflows for developing your problem set solutions:\n\nOpen the qmd file in any editor you like (e.g., Emacs, Sublime, ….). From the command line (we think this will work from a Windows command line such as cmd.exe or PowerShell as well), run quarto preview FILE to show your rendered document live as you edit and save changes. You can put the preview window side by side with your editor, and the preview document should automatically render as you save your qmd file.\nUse VS Code with the following extensions: Python, Quarto, and Jupyter Notebooks. This allows you to execute and preview chunks (and whole document) inside VS Code. This is currently deeb’s favorite path due to how well it integrated with the Python debugger.\nUse RStudio (yes, RStudio), which can manage Python code and will display chunk output in the same way it does with R chunks. This path seems to work quite well and is recommended if you are already familiar with RStudio.\n\nLater in the semester, you may be allowed to work directly in Jupyter notebooks and use quarto to render from them directly. This has a few quirks and limitations, but may be allowed for some problem sets.\nPlease commit your work regularly to your repository as you develop your solutions.",
    "crumbs": [
      "How tos",
      "Problem Set Submissions"
    ]
  },
  {
    "objectID": "howtos/submitPS.html#github-repository",
    "href": "howtos/submitPS.html#github-repository",
    "title": "Problem Set Submissions",
    "section": "GitHub repository",
    "text": "GitHub repository\n\nSetting up your repository\nWe are creating repositories for everyone at github.berkeley.edu. Additionally, homeworks still need to be submitted as PDFs on Gradescope.\nSteps:\n\nLog into github.berkeley.edu using your Berkeley credentials. Because of how the system works, you will need to log in before your account is created. Nothing else needs to be done, just log in and log out.\nAfter accounts are created (may take a couple days after first login), when you log in again, you should see one private repository listed on the left side (e.g., stat243-fall-2024/ahv36). This is your class repository. Do not change the repository settings! They are set up for this class.\nClone the repo to your home directory (I would clone it into a directory just for repositories (e.g., I use ~/repos). In the top-level of your working directory, you should create a file named (exactly) .gitignore.\n\nThe .gitignore file causes Git to ignore transient or computer-specific files that Quarto generates. (more info at https://github.com/github/gitignore) In it, put (again, don’t put dashed lines):\n# cache directories\n/__pycache__\n\n# pickle files\n*.pkl\n*.pickle\n\n\nRepository Organization\nThe problem sets in your repository should be organized into folders with specific filenames.\nWhen we pull from your repository, our code will be assuming the following structure:\nyour_repo/\n├── ps1/\n│   ├── ps1.pdf\n│   ├── ps1.qmd \n│   ├── &lt;possibly auxiliary code or other files&gt;\n├── ps2/\n│   ├── ...\n├── ...\n├── ps8/\n├── .gitignore\n└── info.json\nThe file names are case-sensitive, so please keep everything lowercase.",
    "crumbs": [
      "How tos",
      "Problem Set Submissions"
    ]
  },
  {
    "objectID": "howtos/windowsInstall.html",
    "href": "howtos/windowsInstall.html",
    "title": "R/Rstudio on Windows",
    "section": "",
    "text": "While R was built/designed for UNIX systems, it has been well adapted for Windows. Here, we’ll start with the basics of installing R on Windows. Then, we’ll cover the recommended editor (Rstudio), and how to build pdf documents using MikTeX.\n\n\n\n\n\n\nNote\n\n\n\nThis tutorial installs Windows-only versions of everything. Modern Windows systems have an Ubuntu subsystem available that we highly recommend. See the Installing the Linux Subsystem on Windows tutorial for setting up that configuration.\n\n\n\nInstalling R\nThe first step in installing the R language. This is available on CRAN (Comprehensive R Archive Network).\n\nGo to the CRAN webpage, www.r-project.org\nIn the first paragraph, click the link download R\nYou’re now on a page titled CRAN Mirrors, choose the mirror located closest to your geographic location\n\nMirrors are different servers that all host copies of the same website. You get best performance from the location closest to you.\n\nYou’re now on a paged titled The Comprehensive R Archive Network. The first box is labeled Downloand and Install R, click Download R for Windows\nClick base or install R for the first time, these take you to the same place\n\nFor more advanced things, you may need the Rtools download later. It isn’t necessary now, but remember that for the future.\n\nAt the top is a large-font link, Download R X.X.X for Windows, click this. It will begin downloading the Windows installer for R.\nFollow the instructions for setup. If you are unsure of anything, leave the default settings\n\n\n\nInstalling RStudio\nRStudio is one of the best text editors for coding in R. It is our recommended option for beginning. After you are comfortable with the language, or if you use other languages as well, you may want to explore Atom or Sublime. More advanced options include Emacs with [ESS package][https://ess.r-project.org/] and vim with the Nvim-R plugin.\nTo install RStudio:\n\nGo to the RStudio Desktop download page, rstudio.com/products/rstudio/download/#download\nChoose the download for your OS, most likely the Windows 10/8/7 one\nFollow the instructions for setup. If you are unsure of anything, leave the default settings\nOpen RStudio (R will run automatically in the background)\n\nYou may have to allow RStudio to run if prompted (depends on security settings and anti-virus software)\n\n\nOnce RStudio is installed, you can install or update packages in one of two ways:\n\nVia the console, using install.packages() or update.packages()\n\nVia the gui:\n\nIn the top bar, click on tools\nSelect Install Packages… to install packages\nSelect Check for Package Updates… to update packages\n\n\n\n\nCompiling PDF Documents\nFor the purposes of this class, you will be submitting homeworks as PDF documents that blend written text, code, and code-generated output. These documents are RMarkdown documents, and are dynamic documents that provide a convenient method for documenting your work (more on this in one of the lab sections). To do this, you need a LaTeX renderer. We recommend MiKTeX for Windows.\n\nGo to Getting MiKTeX to download MiKTeX for Windows, miktex.org/download\nThe first page should be Install on Windows, click Download at the bottom of the page\n\nClick the download to begin\n\n\n\n\n\n\n\nImportant\n\n\n\nFOLLOW THESE INSTALL INSTRUCTIONS.\nThe default options are fine in most places, but there is one that will cause problems.\n\n\n\nAccept the Copying Conditions, click next\nInstall only for you, click next\nUse the default directory, click next\nThis should be the Settings page. Under Install missing packages on-the-fly, change the setting to Yes, click next\n\n\n\nBecause we are using MiKTeX as an external renderer, it can’t ask you to install missing packages, and will then fail, so we have to set that installation as automatic.\n\n\nClick start (Optional, but highly recommended) Open RStudio, select a new .Rmd document, d then choose knit. This may take some time, because MiKTeX is installing new braries, but it ensures that your pipeline is setup correctly"
  },
  {
    "objectID": "howtos/installQuarto.html",
    "href": "howtos/installQuarto.html",
    "title": "Installing and Using Quarto",
    "section": "",
    "text": "Unless you plan to generate your problem set solutions on the SCF, you’ll need to install Quarto.\nOnce installed, you should be able to run commands such as quarto render FILE and quarto preview FILE from the command line.\nQuarto also runs from the Windows Command shell or PowerShell. We’ll add details/troubleshooting tips here as needed.",
    "crumbs": [
      "How tos",
      "Installing and Using Quarto"
    ]
  }
]